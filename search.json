[
  {
    "objectID": "measurementinvariance.html#loading-the-r-packages",
    "href": "measurementinvariance.html#loading-the-r-packages",
    "title": "6  Measurement Invariance",
    "section": "6.1 Loading the R packages",
    "text": "6.1 Loading the R packages\nIn this lab, we will use the lavaan and semTools packages, which we have already installed in earlier labs. So, we can simply get those packages from the R package library:\n\nlibrary(lavaan)\nlibrary(semTools)"
  },
  {
    "objectID": "measurementinvariance.html#loading-data-into-our-environment",
    "href": "measurementinvariance.html#loading-data-into-our-environment",
    "title": "6  Measurement Invariance",
    "section": "6.2 Loading data into our environment",
    "text": "6.2 Loading data into our environment\nFor this lab, we will use a dataset that contains 3811 item responses to 10 items about financial well-being. You can download the data by right-clicking this link and selecting “Save Link As…” in the drop-down menu: data/finance.csv. Make sure to save it in the folder you are using for this class.\n\nfinance &lt;- read.csv(\"data/finance.csv\")\n\nWe can look at the variables in the dataset using describe() from the psych package. In addition to the 10 items, the dataset also includes a variable denoting whether a participant worked in the public or private sector.\n\npsych::describe(finance, skew = FALSE)\n\n        vars    n mean   sd median min max range   se\nitem1      1 3811 2.89 1.23      3   1   5     4 0.02\nitem2      2 3811 3.09 1.10      3   1   5     4 0.02\nitem3      3 3811 2.69 1.19      3   1   5     4 0.02\nitem4      4 3811 3.15 1.04      3   1   5     4 0.02\nitem5      5 3811 2.87 1.24      3   1   5     4 0.02\nitem6      6 3811 3.25 1.13      3   1   5     4 0.02\nitem7      7 3811 2.48 1.19      2   1   5     4 0.02\nitem8      8 3811 3.27 1.25      3   1   5     4 0.02\nitem9      9 3811 2.22 1.14      2   1   5     4 0.02\nitem10    10 3811 2.83 1.13      3   1   5     4 0.02\nsector*   11 3811 1.55 0.50      2   1   2     1 0.01\n\n\nThese items make up the Financial Well-Being scale, which was developed by the Consumer Financial Protection Bureau (CFPB):\n\nI could handle a major unexpected expense (P)\nI am securing my financial future (P)\nBecause of my money situation, I will never have the things I want in life (N)\nI can enjoy life because of the way I’m managing my money (P)\nI am just getting by financially (N)\nI am concerned that the money I have or will save won’t last (N)\nGiving a gift would put a strain on my finances for the month (N)\nI have money left over at the end of the month (P)\nI am behind with my finances (N)\nMy finances control my life (N)\n\nThe items measure positive (P) and negative (N) financial well-being."
  },
  {
    "objectID": "measurementinvariance.html#the-theoretical-measurement-model",
    "href": "measurementinvariance.html#the-theoretical-measurement-model",
    "title": "6  Measurement Invariance",
    "section": "6.3 The Theoretical Measurement Model",
    "text": "6.3 The Theoretical Measurement Model\nNext, we will set up the theoretical measurement model representing the hypothesized internal structure of this measure. In this case, the construct of financial well-being is represented by two correlated sub-constructs: financial stability and financial instability.\n\n# Two-factor CFA model\nfinancemodel &lt;- \"positive =~ item1 + item2 + item4 + item8\n                 negative =~ item3 + item5 + item6 + item7 + item9 + item10\""
  },
  {
    "objectID": "measurementinvariance.html#step-1-configural-invariance",
    "href": "measurementinvariance.html#step-1-configural-invariance",
    "title": "6  Measurement Invariance",
    "section": "6.4 Step 1: Configural Invariance",
    "text": "6.4 Step 1: Configural Invariance\nTo test measurement invariance (MI), we will use the lavaan package. First, we will assess configural invariance, which we can do using the code below. The main difference from previous CFAs is that we now use the group = \"sector\" argument to denote which variable in our data denotes what group a participant is a member of. In addition, we will tell lavaan to scale the latent factors so they have a mean of 0 and a standard deviation of 1, using the std.lv = TRUE argument (note that there are other methods for scaling the latent variable, but going into those technical details is beyond the scope of this class):\n\n# Configural model\nconfig &lt;- cfa(model = financemodel, data = finance, \n              std.lv = TRUE,\n              group = \"sector\")\n\nTo evaluate whether configural invariance holds, we can look at the summary output. In this case, I’m mostly interested in looking at the fit of the model to the data, so we will use estimates = F to omit the parameter estimates from the summary output.\n\nsummary(config, fit.measures = T, estimates = F)\n\nlavaan 0.6.17 ended normally after 27 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        62\n\n  Number of observations per group:                   \n    public                                        2110\n    private                                       1701\n\nModel Test User Model:\n                                                      \n  Test statistic                               875.170\n  Degrees of freedom                                68\n  P-value (Chi-square)                           0.000\n  Test statistic for each group:\n    public                                     465.024\n    private                                    410.146\n\nModel Test Baseline Model:\n\n  Test statistic                             20856.369\n  Degrees of freedom                                90\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.961\n  Tucker-Lewis Index (TLI)                       0.949\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)             -49649.240\n  Loglikelihood unrestricted model (H1)     -49211.655\n                                                      \n  Akaike (AIC)                               99422.480\n  Bayesian (BIC)                             99809.711\n  Sample-size adjusted Bayesian (SABIC)      99612.704\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.079\n  90 Percent confidence interval - lower         0.074\n  90 Percent confidence interval - upper         0.084\n  P-value H_0: RMSEA &lt;= 0.050                    0.000\n  P-value H_0: RMSEA &gt;= 0.080                    0.359\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.031\n\n\nOverall, the fit of the model to the data, allowing all parameters to be freely estimated across groups, is decent: CFI &gt; .95, TLI = .95, RMSEA = .079, 95% CI [.074, .084] SRMR &lt; .08. Not ideal in terms of RMSEA, but otherwise okay. More importantly, this output shows us what part of the overall Model Chi-square is stemming from each of the two groups (under Test statistic for each group). If those two numbers are relatively equal, then the model fits about equally well to each group’s data. If you notice that the Chi-square contribution is much larger for one group than the other, it is an indication that you may not be able to conclude that there is configural invariance. In this case, both groups contribute about equally to the total Chi-square, indicating similar model-data fit across groups.\n\nIf configural invariance is not supported\nIf configural invariance is not supported, then you can start your investigation by looking at the estimates across each group to see if there are noticeable issues (e.g., a lot of low factor loadings in one group). You can follow this investigation up by running separate EFAs for each group, to examine what kind of factor structure emerges for each group. The code below shows you how to start this investigation for this example dataset, but I do not include the output to reduce the length of this (already lengthy) lab:\n\n# Examine parameter estimates\nsummary(config)\n\n# Split data is two\npublic &lt;- subset(finance, sector == \"public\")\nprivate &lt;- subset(finance, sector == \"private\")\n\n# Run parallel analysis for each group \n# (can be followed up by full EFA examination)\nlibrary(psych)\nfa.parallel(public[,1:10], fa = \"fa\")\nfa.parallel(private[,1:10], fa = \"fa\")"
  },
  {
    "objectID": "measurementinvariance.html#step-2-metric-invariance",
    "href": "measurementinvariance.html#step-2-metric-invariance",
    "title": "6  Measurement Invariance",
    "section": "6.5 Step 2: Metric Invariance",
    "text": "6.5 Step 2: Metric Invariance\nNext, we will estimate the metric invariance model. To do so, we again use the cfa() function and specify our grouping variable and latent factor scale. In addition, we will add group.equal = \"loadings\":\n\n# Metric model\nmetric &lt;- cfa(model = financemodel, data = finance, \n              group = \"sector\", \n              std.lv = TRUE,\n              group.equal = \"loadings\")\n\nTo see if the metric model fits the data significantly worse than the configural model, we will use the compareFit() function from the semTools package:\n\nsummary(compareFit(config, metric))\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n       Df   AIC   BIC  Chisq Chisq diff    RMSEA Df diff Pr(&gt;Chisq)\nconfig 68 99422 99810 875.17                                       \nmetric 76 99417 99754 885.35     10.179 0.011957       8     0.2527\n\n####################### Model Fit Indices ###########################\n          chisq df pvalue rmsea   cfi   tli  srmr        aic        bic\nconfig 875.170† 68   .000 .079  .961† .949  .031† 99422.480  99809.711 \nmetric 885.349  76   .000 .075† .961  .954† .033  99416.660† 99753.925†\n\n################## Differences in Fit Indices #######################\n                df  rmsea cfi   tli  srmr    aic     bic\nmetric - config  8 -0.004   0 0.005 0.002 -5.821 -55.786\n\n\nThe null hypothesis that we’re testing is that the fit of the metric and configural model is equivalent (i.e., adding equality constraints to the loadings does not make the model fit worse). Thus, if the p-value associated with the Chi-square difference test is &gt; .05, we can retain that null hypothesis and conclude that metric invariance holds for these data. If the p-value associated with the Chi-square difference test is &lt; .05, then we need to reject the null hypothesis and conclude that the metric invariance model fit the data significantly worse, and so metric invariance does not hold.\nNote: If any loadings were found to be non-invariant (i.e., there is partial metric invariance), then the intercepts of those items also need to be estimated freely across groups in the next step. In other words, you start your investigation with a model that is already partially invariant at the scalar level (see below how to run partial invariance models)."
  },
  {
    "objectID": "measurementinvariance.html#step-3-scalar-invariance",
    "href": "measurementinvariance.html#step-3-scalar-invariance",
    "title": "6  Measurement Invariance",
    "section": "6.6 Step 3: Scalar Invariance",
    "text": "6.6 Step 3: Scalar Invariance\nNext, we will evaluate if scalar invariance holds for our data. To do so, we simply add the intercepts to the group.equal = c(\"loadings\",\"intercepts\") argument:\n\n# Scalar model\nscalar &lt;- cfa(model = financemodel, data = finance, \n              group = \"sector\", \n              std.lv = TRUE,\n              group.equal = c(\"loadings\",\"intercepts\"))\n\nNow, we compare the scalar model’s fit to the fit of the metric model, to see if adding the equality constraints on the intercepts results in significantly worse fit:\n\nsummary(compareFit(metric, scalar))\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n       Df   AIC   BIC  Chisq Chisq diff    RMSEA Df diff Pr(&gt;Chisq)    \nmetric 76 99417 99754 885.35                                           \nscalar 84 99453 99740 937.72     52.371 0.053951       8  1.427e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n####################### Model Fit Indices ###########################\n          chisq df pvalue rmsea   cfi   tli  srmr        aic        bic\nmetric 885.349† 76   .000 .075  .961† .954  .033† 99416.660† 99753.925 \nscalar 937.720  84   .000 .073† .959  .956† .034  99453.030  99740.330†\n\n################## Differences in Fit Indices #######################\n                df  rmsea    cfi   tli  srmr    aic     bic\nscalar - metric  8 -0.002 -0.002 0.002 0.001 36.371 -13.595\n\n\nFrom the Chi-square difference test above, we can see that the scalar model fit the data significantly worse than the metric model. This means that, at least for some items, the expected response for those with an average (i.e., 0) score on the latent factor differs across groups.\n\nStep 3B: Partial Scalar Invariance\nTo easily get an overview of what equality constraints should be released to result in the largest improvement in model fit, we can use the lavTestScore function from the lavaan package:\n\n# Adjust the model for partial invariance testing\nlavTestScore(scalar)\n\n$test\n\ntotal score test:\n\n   test     X2 df p.value\n1 score 62.622 20       0\n\n$uni\n\nunivariate score tests:\n\n     lhs op   rhs     X2 df p.value\n1   .p1. == .p36.  0.125  1   0.724\n2   .p2. == .p37.  0.594  1   0.441\n3   .p3. == .p38.  0.323  1   0.570\n4   .p4. == .p39.  0.327  1   0.567\n5   .p5. == .p40.  1.733  1   0.188\n6   .p6. == .p41.  1.909  1   0.167\n7   .p7. == .p42.  1.219  1   0.270\n8   .p8. == .p43.  2.986  1   0.084\n9   .p9. == .p44.  2.129  1   0.144\n10 .p10. == .p45.  0.011  1   0.916\n11 .p24. == .p59. 15.296  1   0.000\n12 .p25. == .p60.  0.215  1   0.643\n13 .p26. == .p61. 16.963  1   0.000\n14 .p27. == .p62.  0.257  1   0.612\n15 .p28. == .p63.  9.145  1   0.002\n16 .p29. == .p64.  0.445  1   0.505\n17 .p30. == .p65.  0.101  1   0.751\n18 .p31. == .p66. 21.472  1   0.000\n19 .p32. == .p67.  1.433  1   0.231\n20 .p33. == .p68.  4.348  1   0.037\n\n\nTo figure out what equality constraints map onto which item’s intercept, we can look at the parameter table of the scalar model. In the code below, I filter the output to only include intercept parameters (op = \"~1\"), only show parameters from group 2 (group == 2) and then to only include certain columns and rows of that output. This was mostly done to keep this PDF from becoming too large. You don’t need to do any of this filtering yourself.\n\n# To view the entire parameter table, simply use this code \n# (remove the hashtag in front of the next line):\n# parTable(scalar)\n\n# To filter the output (this literal code will only work for \n# this example):\nsubset(parTable(scalar), op == \"~1\" & group == 2)[,c(1:4, 11:15)]\n\n   id      lhs op rhs label plabel start    est    se\n59 59    item1 ~1     .p24.  .p59. 2.733  2.994 0.025\n60 60    item2 ~1     .p25.  .p60. 2.978  3.173 0.022\n61 61    item4 ~1     .p26.  .p61. 3.093  3.232 0.021\n62 62    item8 ~1     .p27.  .p62. 3.162  3.370 0.025\n63 63    item3 ~1     .p28.  .p63. 2.708  2.638 0.024\n64 64    item5 ~1     .p29.  .p64. 2.931  2.820 0.024\n65 65    item6 ~1     .p30.  .p65. 3.301  3.203 0.022\n66 66    item7 ~1     .p31.  .p66. 2.600  2.432 0.024\n67 67    item9 ~1     .p32.  .p67. 2.256  2.174 0.022\n68 68   item10 ~1     .p33.  .p68. 2.855  2.785 0.022\n69 69 positive ~1            .p69. 0.000 -0.217 0.035\n70 70 negative ~1            .p70. 0.000  0.121 0.035\n\n\nIn the table above, we can see the the intercept of Item 7 is associated with the largest potential improvement in model fit. So, we estimate a partial scalar invariance model. To release the equality constraint for the intercept of Item 7, we add the group.partial = c(\"item7 ~ 1\") argument. Once the model is estimated, we will compare its fit to the metric model, to see if we need to release additional intercept parameters:\n\nscalar2 &lt;- cfa(model = financemodel, data = finance, \n               group = \"sector\",\n               std.lv = TRUE,\n               group.equal = c(\"loadings\",\"intercepts\"), \n               group.partial = c(\"item7 ~ 1\")) \n# for a loading, group.partial would look like: \"negative =~ item7\"\n\nsummary(compareFit(metric, scalar2))\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n        Df   AIC   BIC  Chisq Chisq diff    RMSEA Df diff Pr(&gt;Chisq)    \nmetric  76 99417 99754 885.35                                           \nscalar2 83 99434 99727 916.21     30.863 0.042297       7   6.59e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n####################### Model Fit Indices ###########################\n           chisq df pvalue rmsea   cfi   tli  srmr        aic        bic\nmetric  885.349† 76   .000 .075  .961† .954  .033† 99416.660† 99753.925 \nscalar2 916.212  83   .000 .073† .960  .956† .034  99433.523  99727.068†\n\n################## Differences in Fit Indices #######################\n                 df  rmsea    cfi   tli  srmr    aic     bic\nscalar2 - metric  7 -0.002 -0.001 0.003 0.001 16.863 -26.857\n\n\nThe fit of the first partial scalar model is still worse than the metric model. So, we need to release additional equality constraints. After examining the modification indices again, we release the equality constraint of Item 4’s intercept, estimate the model again, and compare this second partial model to the metric model:\n\n# Adjust the model for partial invariance testing\nlavTestScore(scalar2)\n\n$test\n\ntotal score test:\n\n   test     X2 df p.value\n1 score 41.208 19   0.002\n\n$uni\n\nunivariate score tests:\n\n     lhs op   rhs     X2 df p.value\n1   .p1. == .p36.  0.125  1   0.723\n2   .p2. == .p37.  0.594  1   0.441\n3   .p3. == .p38.  0.323  1   0.570\n4   .p4. == .p39.  0.327  1   0.567\n5   .p5. == .p40.  2.131  1   0.144\n6   .p6. == .p41.  1.823  1   0.177\n7   .p7. == .p42.  1.128  1   0.288\n8   .p8. == .p43.  1.924  1   0.165\n9   .p9. == .p44.  1.885  1   0.170\n10 .p10. == .p45.  0.045  1   0.832\n11 .p24. == .p59. 15.297  1   0.000\n12 .p25. == .p60.  0.215  1   0.643\n13 .p26. == .p61. 16.964  1   0.000\n14 .p27. == .p62.  0.257  1   0.612\n15 .p28. == .p63.  2.901  1   0.089\n16 .p29. == .p64.  2.900  1   0.089\n17 .p30. == .p65.  1.991  1   0.158\n18 .p32. == .p67.  0.000  1   0.989\n19 .p33. == .p68.  0.840  1   0.359\n\nscalar3 &lt;- cfa(model = financemodel, data = finance, \n               group = \"sector\", \n               std.lv = TRUE,\n               group.equal = c(\"loadings\",\"intercepts\"), \n               group.partial = c(\"item7 ~ 1\", \"item4 ~ 1\"))\n\nsummary(compareFit(metric, scalar3))\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n        Df   AIC   BIC  Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)  \nmetric  76 99417 99754 885.35                                        \nscalar3 82 99418 99718 899.16     13.812 0.02614       6     0.0318 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n####################### Model Fit Indices ###########################\n           chisq df pvalue rmsea   cfi   tli  srmr        aic        bic\nmetric  885.349† 76   .000 .075  .961† .954  .033† 99416.660† 99753.925 \nscalar3 899.161  82   .000 .072† .961  .957† .033  99418.472  99718.263†\n\n################## Differences in Fit Indices #######################\n                 df  rmsea cfi   tli srmr   aic     bic\nscalar3 - metric  6 -0.002   0 0.003    0 1.812 -35.661\n\n\nThe fit of this partial model is still worse than the metric model. So, we need to release additional equality constraints. After examining the modification indices again, we release the equality constraint of Item 1’s intercept, estimate the model again, and compare this second partial model to the metric model:\n\n# Adjust the model for partial invariance testing\nlavTestScore(scalar3)\n\n$test\n\ntotal score test:\n\n   test     X2 df p.value\n1 score 24.243 18   0.147\n\n$uni\n\nunivariate score tests:\n\n     lhs op   rhs    X2 df p.value\n1   .p1. == .p36. 0.276  1   0.599\n2   .p2. == .p37. 0.946  1   0.331\n3   .p3. == .p38. 1.352  1   0.245\n4   .p4. == .p39. 0.127  1   0.722\n5   .p5. == .p40. 2.132  1   0.144\n6   .p6. == .p41. 1.826  1   0.177\n7   .p7. == .p42. 1.128  1   0.288\n8   .p8. == .p43. 1.919  1   0.166\n9   .p9. == .p44. 1.883  1   0.170\n10 .p10. == .p45. 0.046  1   0.830\n11 .p24. == .p59. 6.551  1   0.010\n12 .p25. == .p60. 0.744  1   0.388\n13 .p27. == .p62. 3.488  1   0.062\n14 .p28. == .p63. 2.901  1   0.089\n15 .p29. == .p64. 2.900  1   0.089\n16 .p30. == .p65. 1.991  1   0.158\n17 .p32. == .p67. 0.000  1   0.989\n18 .p33. == .p68. 0.840  1   0.359\n\nscalar4 &lt;- cfa(model = financemodel, data = finance, \n               group = \"sector\", \n               std.lv = TRUE,\n               group.equal = c(\"loadings\",\"intercepts\"), \n               group.partial = c(\"item7 ~ 1\", \"item4 ~ 1\", \n                                 \"item1 ~ 1\"))\n\nsummary(compareFit(metric, scalar4))\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n        Df   AIC   BIC  Chisq Chisq diff    RMSEA Df diff Pr(&gt;Chisq)\nmetric  76 99417 99754 885.35                                       \nscalar4 81 99414 99720 892.61     7.2647 0.015418       5     0.2017\n\n####################### Model Fit Indices ###########################\n           chisq df pvalue rmsea   cfi   tli  srmr        aic        bic\nmetric  885.349† 76   .000 .075  .961† .954  .033† 99416.660  99753.925 \nscalar4 892.614  81   .000 .073† .961  .957† .033  99413.925† 99719.961†\n\n################## Differences in Fit Indices #######################\n                 df  rmsea cfi   tli srmr    aic     bic\nscalar4 - metric  5 -0.002   0 0.003    0 -2.735 -33.964\n\n\nThis time, the fit of the partial scalar model is not worse than that of the metric model (p &gt; .05), so we can conclude that partial scalar invariance holds for these data."
  },
  {
    "objectID": "measurementinvariance.html#step-4-strict-invariance-optional",
    "href": "measurementinvariance.html#step-4-strict-invariance-optional",
    "title": "6  Measurement Invariance",
    "section": "6.7 Step 4 Strict Invariance (Optional)",
    "text": "6.7 Step 4 Strict Invariance (Optional)\nFinally, we can examine strict invariance by constraining the residuals to be equal across groups. To do so, we simply add the residuals to the group.equal = c(\"loadings\",\"intercepts\",\"residuals) argument. Note that we keep the partial intercepts from the previous step and need to add partial residuals for those items.\n\n#Strict model\nstrict &lt;- cfa(model = financemodel, data = finance, \n              group = \"sector\", \n              std.lv = TRUE,\n              group.equal = c(\"loadings\",\"intercepts\",\"residuals\"), \n              group.partial = c(\"item7 ~ 1\", \"item4 ~ 1\", \n                                \"item1 ~ 1\",\n                                \"item7 ~~ item7\", \n                                \"item4 ~~ item4\",\n                                \"item1 ~~ item1\"))\n\nSimilar to previous steps, we can compare the fit of this model to the previous (partial scalar) model:\n\nsummary(compareFit(scalar4, strict))\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n        Df   AIC   BIC  Chisq Chisq diff    RMSEA Df diff Pr(&gt;Chisq)   \nscalar4 81 99414 99720 892.61                                          \nstrict  88 99423 99686 915.96     23.351 0.035012       7    0.00148 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n####################### Model Fit Indices ###########################\n           chisq df pvalue rmsea   cfi   tli  srmr        aic        bic\nscalar4 892.614† 81   .000 .073  .961† .957  .033† 99413.925† 99719.961 \nstrict  915.965  88   .000 .070† .960  .959† .033  99423.276  99685.593†\n\n################## Differences in Fit Indices #######################\n                 df  rmsea    cfi   tli srmr   aic     bic\nstrict - scalar4  7 -0.002 -0.001 0.003    0 9.351 -34.368"
  },
  {
    "objectID": "measurementinvariance.html#step-5-interpreting-the-mean-difference-between-public-and-private-sector-groups-optional",
    "href": "measurementinvariance.html#step-5-interpreting-the-mean-difference-between-public-and-private-sector-groups-optional",
    "title": "6  Measurement Invariance",
    "section": "6.8 Step 5: Interpreting the Mean Difference between Public and Private Sector Groups (Optional)",
    "text": "6.8 Step 5: Interpreting the Mean Difference between Public and Private Sector Groups (Optional)\nAs we’ve been able to establish partial scalar invariance, we can compare the latent factor means across groups. To do so, we use the final partial scalar model:\n\nsubset(parameterEstimates(scalar4), \n       (op == \"~1\" & (lhs == \"positive\" | lhs == \"negative\")))\n\n        lhs op rhs block group label    est    se      z pvalue ci.lower\n34 positive ~1         1     1        0.000 0.000     NA     NA    0.000\n35 negative ~1         1     1        0.000 0.000     NA     NA    0.000\n69 positive ~1         2     2       -0.216 0.037 -5.823  0.000   -0.289\n70 negative ~1         2     2        0.086 0.035  2.425  0.015    0.016\n   ci.upper\n34    0.000\n35    0.000\n69   -0.143\n70    0.155\n\n\nTo help with interpretation, the means of the latent factors in the first group (Public) are fixed to 0 and their variances are fixed to one. This constraint means that the freely estimated latent means in the second group (Private) can be interpreted as “relative to” the first group. In the output, we can see that the positive financial well-being mean is negative (and significant) whereas the negative financial well-being mean is positive (and significant). However, the factors’ variances in this group are not equal to 1 (they are slightly smaller) and so these means are not measured on the same scale as the means of the Public sector group. There are several ways to make the means comparable.\n\nStep 5: Method 1 for making mean differences comparable\nThe first is to evaluate whether the latent factor variances can be constrained to equivalence without resulting in worse model fit. This would place both groups’ factors on the standardized scale (variance or sd = 1) and mean differences can be interpreted in terms of standard deviation units. To test this model, we can use the following code, adding \"lv.variances\" to the group.equal argument and then testing whether the resulting model fit significantly worse than the partial scalar model. Note that we do not have to meet the strict invariance level to test this latent variance level of invariance.\n\nlvvar &lt;- cfa(model = financemodel, data = finance, \n               group = \"sector\", \n               std.lv = TRUE,\n               group.equal = c(\"loadings\",\"intercepts\", \n                               \"lv.variances\"), \n               group.partial = c(\"item7 ~ 1\", \"item4 ~ 1\", \n                                 \"item1 ~ 1\"))\n\nsummary(compareFit(scalar4, lvvar))\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n        Df   AIC   BIC  Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq)\nscalar4 81 99414 99720 892.61                                    \nlvvar   83 99410 99704 893.05    0.43956     0       2     0.8027\n\n####################### Model Fit Indices ###########################\n           chisq df pvalue rmsea   cfi   tli  srmr        aic        bic\nscalar4 892.614† 81   .000 .073  .961  .957  .033† 99413.925  99719.961 \nlvvar   893.053  83   .000 .072† .961† .958† .034  99410.364† 99703.910†\n\n################## Differences in Fit Indices #######################\n                df  rmsea cfi   tli  srmr   aic     bic\nlvvar - scalar4  2 -0.001   0 0.001 0.001 -3.56 -16.052\n\n\nConstraining the latent factor variances to equivalence across groups does not result in significantly worse model fit (p = .803), so we can use the estimates from the lvvar output to interpret mean differences between the Public and Private sector groups.\n\nsubset(parameterEstimates(lvvar), \n       (op == \"~1\" & (lhs == \"positive\" | lhs == \"negative\")))\n\n        lhs op rhs block group label    est    se      z pvalue ci.lower\n34 positive ~1         1     1        0.000 0.000     NA     NA    0.000\n35 negative ~1         1     1        0.000 0.000     NA     NA    0.000\n69 positive ~1         2     2       -0.217 0.037 -5.829  0.000   -0.290\n70 negative ~1         2     2        0.087 0.036  2.422  0.015    0.016\n   ci.upper\n34    0.000\n35    0.000\n69   -0.144\n70    0.157\n\n\nBased on the results above, we can conclude that positive financial well-being is .22 standard deviations (SE = .04) lower for those working in the Private sector compared to those working in the Public section (p &lt; .001). In addition, negative financial well-being is .09 standard deviations (SE = .04) higher for those working in the Private sector compared to those working in the Public section (p = .015).\n\n\nStep 5: Method 2 for making mean differences comparable\nIf the previous method would have shown that factor variances are not comparable, then we could have used the formula for standardized mean differences (Cohen’s D) to compute mean differences.\n\\[Cohen's D = \\frac{(m_1 - m_2)}{\\sqrt\\sigma^2}\\]\nTo do so, we need the latent factor means and variances from the partial scalar model. We already know that the means and variances in the first group are 0 and 1 respectively, so we only need to know the means and variances for group 2 (i.e., Private sector):\n\nsubset(parameterEstimates(scalar4), \n       (group == 2 & \n          (op == \"~1\" | op == \"~~\") & (lhs == \"positive\" | lhs == \"negative\")))\n\n        lhs op      rhs block group label    est    se       z pvalue ci.lower\n56 positive ~~ positive     2     2        0.973 0.051  19.042  0.000    0.873\n57 negative ~~ negative     2     2        0.967 0.051  18.882  0.000    0.867\n58 positive ~~ negative     2     2       -0.818 0.042 -19.583  0.000   -0.899\n69 positive ~1              2     2       -0.216 0.037  -5.823  0.000   -0.289\n70 negative ~1              2     2        0.086 0.035   2.425  0.015    0.016\n   ci.upper\n56    1.074\n57    1.068\n58   -0.736\n69   -0.143\n70    0.155\n\n\nWhen variances are not equal across groups, we can compute separate standardized mean differences using each group’s variance in the denominator, which will give us a range of plausible mean difference effect sizes (not to be confused with a confidence interval!). Here, we first compute the Cohen’s D using the first group’s variance (= 1):\n\n# positive\n((-.216) - 0) / sqrt(1)\n\n[1] -0.216\n\n# negative\n((.086) - 0) / sqrt(1)\n\n[1] 0.086\n\n\nNext, we compute the Cohen’s D using the second group’s variance estimates:\n\n# positive\n((-.216) - 0) / sqrt(.973)\n\n[1] -0.2189764\n\n# negative\n((.086) - 0) / sqrt(.967)\n\n[1] 0.08745511\n\n\nSo, depending on the standardizer (which variance) used, the Cohen’s D for positive financial well-being is approximately -.217 to -.219, and the Cohen’s D for negative financial well-being is approximately .086 to .087. Note that the ranges here are really narrow because the variances are so similar, the range would be larger if the difference in the variances was larger."
  },
  {
    "objectID": "measurementinvariance.html#summary",
    "href": "measurementinvariance.html#summary",
    "title": "6  Measurement Invariance",
    "section": "6.9 Summary",
    "text": "6.9 Summary\nIn this R lab, you were introduced to the steps involved in measurement invariance testing, an important quantitative method that can help us collect evidence regarding the fairness of our measurement scale. You also learned how to compare means of latent variables, using several different approaches. This is the final R lab of this class!"
  },
  {
    "objectID": "gettingstarted.html#installing-new-software",
    "href": "gettingstarted.html#installing-new-software",
    "title": "1  Getting Started",
    "section": "1.1 Installing New Software",
    "text": "1.1 Installing New Software\nAs the title of this page suggests, all labs will be done using R (and RStudio). To use these programs, you’ll need to install both R and RStudio. Follow the instructions below to install them.\n\nStep 1: Install R\nR is a programming language and computing environment specialized for statistical analysis and data manipulation. It’s commonly used for performing statistical tests, creating data visualizations, and writing data analysis reports.\n\nInstalling R for Windows Computers\nGo to https://cloud.r-project.org/bin/windows/base/ and click the link titled Download R-4.3.2 for Windows (note: the version number might be different, but the remainder of the link will be the same). This will download the R Installer into your Downloads folder, where you can double click on it and follow the prompts on the screen to finish installing R. You can accepts all default settings.\n\n\nInstalling R for Mac Computers\nYou will need to figure out if you have an Intel Processor or an Apple M Processor. You can do so by clicking on the Apple icon in the top-left corner of your screen and clicking on About this Mac. The window that will pop up will show you an overview of your computer, including the processor/chip used.\nOnce you know what processor your computer has, go to https://cloud.r-project.org/bin/macosx/, and:\n\nIf your computer has an Intel Processor, click on the file titled R-4.3.2-x86_64.pkg\nIf your computer has an Apple M Processor, click on the file titled R-4.3.2-arm64.pkg\n\nNote: the version number might be different, but the remainder of the link will be the same. This will download the R Installer into your Downloads folder, where you can double click on it and follow the prompts on the screen to finish installing R. You can accepts all default settings.\n\n\nInstalling R for Linux Computers\nIf you are using a Linux-based operating system, use your system’s package manager to install R. For example, here are the instructions for installing R on Ubuntu.\n\n\n\n\n\n\nNote\n\n\n\nR cannot be installed on Chromebooks, so you’ll need to use the computers available in the classroom/computer labs.\n\n\n\n\n\nStep 2: Install RStudio\nRStudio is an integrated development environment (IDE) for reproducible scientific computing that is developed for the R programming language. An IDE is basically a nicer-looking user interface that can be customized to suit the preferences of the user. This is the actual program that we will use in class!\n\nDownload the latest, free version of RStudio Desktop. Be sure to get the version that is appropriate for your operating system.\nInstall RStudio Desktop by launching the installer after it downloads. You can accept all the defaults during installation.\n\n\n\n\n\n\n\nTip\n\n\n\nFor more detailed instructions for downloading and installing R and RStudio, you can watch this video tutorial on YouTube. To learn about (or review) R basics, you can skim this (free!) book by Navarro (2015): Learning Statistics with R. There is also the SWIRL Interactive R Tutorial that lets you learn about the basics of R while using R."
  },
  {
    "objectID": "gettingstarted.html#install-necessary-packages",
    "href": "gettingstarted.html#install-necessary-packages",
    "title": "1  Getting Started",
    "section": "1.2 Install Necessary Packages",
    "text": "1.2 Install Necessary Packages\nThroughout these labs, we will rely on a set of R packages, which add functionality to the base R language (like expansion sets of a game). These packages are typically available through CRAN or GitHub. You only need to install packages once (but you may need to update them!), so lets do that now.\nWe will start with a set of packages that we can download from CRAN, using the built-in install.packages function:\n\ninstall.packages(c(\"rio\", \"ggplot2\", \"psych\",\"correlation\", \n                   \"GPArotation\", \"lavaan\", \"MBESS\", \n                    \"devtools\"))\n\nRunning the code above will install:\n\nrio: makes importing lots of different data file types easy.\nggplot2: a versatile visualization package.\npsych: will help us cover topics such as exploratory factor analysis and reliability.\ncorrelation: includes fancy correlation coeficients\nGPArotation: helps with exploratory factor analysis\nlavaan: the main structural equation modeling package we will use to cover confirmatory factor analysis and measurement invariance.\nMBESS: includes additional internal consistency measures\ndevtools: a package that helps us install packages that are available on GitHub.\n\n\n\n\n\n\n\nTip\n\n\n\nIf you experience issues installing the MBESS package on macOS, you likely need to install a few additional tools. Go to this page to download and install those tools: Compile Tools for macOS.\n\n\nIn addition to these main packages, R might also install additional packages that are needed for these 8 packages to work (so-called dependents).\nNext, you will install a package, semTools from GitHub. Due to a bug in the version of this package on CRAN, we need to use the unofficial, development version of the package. You may need to uncomment (remove the #) the first line of code and execute both lines for this to work. In some cases, the download of the semTools package is too slow and results in an error because the R session times out.\n\n# options(timeout = max(300, getOption(\"timeout\")))\ndevtools::install_github(\"simsem/semTools/semTools\")"
  },
  {
    "objectID": "gettingstarted.html#data-used-in-the-r-labs",
    "href": "gettingstarted.html#data-used-in-the-r-labs",
    "title": "1  Getting Started",
    "section": "1.3 Data Used in the R Labs",
    "text": "1.3 Data Used in the R Labs\nSeveral of the R Labs require you to download data files to use for the analyses. Links to these data files are included within each lab, accompanied by an explanation and citation.\nYou are now ready to continue to the second R Lab, where you will learn all about correlation coefficients."
  },
  {
    "objectID": "correlations.html#loading-r-packages",
    "href": "correlations.html#loading-r-packages",
    "title": "2  Correlations",
    "section": "2.1 Loading R Packages",
    "text": "2.1 Loading R Packages\nIf you want to use the functionality of a package, you will need to “load” the package into your environment. To do that, we use the library function:\n\nlibrary(rio)\nlibrary(psych)\nlibrary(ggplot2)\nlibrary(correlation)"
  },
  {
    "objectID": "correlations.html#loading-data",
    "href": "correlations.html#loading-data",
    "title": "2  Correlations",
    "section": "2.2 Loading Data",
    "text": "2.2 Loading Data\nYou can download the data by right-clicking this link and selecting “Save Link As…” in the drop-down menu: data/tempice.csv. Make sure to save it in the folder you are using for this class.\nTypically, you will import some data file into your R environment for further analysis. There are many ways of doing this. I will show you two:\n\nYou can use a point-and-click approach by clicking the Import Dataset button in the right-top window.\nYou can use a function (the one we use is from the rio package).\n\n\ntempice &lt;- import(file = \"data/tempice.csv\")\n\nThe function above will attempt to import the file tempice.csv from a folder called data, which is located inside your working directory.\nSometimes, running the code above doesn’t work because R thinks you want to import the data from the wrong folder (which R calls the working directory). We can check what the working directory is:\n\ngetwd()\n\nIf the result of this function is not the folder containing your data file, then you can change the working directory in two ways:\n\nUse a point-and-click approach by moving your cursor to the bottom-right window to navigate to the correct folder (in the Files tab).\nUse the following R function to change the working directory:\n\n\n# Mac OS:\nsetwd(\"~/Dropbox/Work/Teaching/Measurement/R Labs\") \n\n# Windows:\nsetwd(\"C:/Users/sonja/Dropbox/Work/Teaching/Measurement/R Labs\") \n\n# Note: the folder that you are using for this class will very \n# likely be in a different location. \n\nTypically, R/RStudio will set the working directory to the folder containing the R file you open. If you start RStudio by itself (instead of opening a file), then the working directory will typically be set to your home folder."
  },
  {
    "objectID": "correlations.html#basic-r-operations",
    "href": "correlations.html#basic-r-operations",
    "title": "2  Correlations",
    "section": "2.3 Basic R operations",
    "text": "2.3 Basic R operations\nBelow are some basic operations that you can execute in R. First, you can use R as a fancy calculator:\n\n1 + 1\n\n[1] 2\n\n5 / 3.21\n\n[1] 1.557632\n\n4*4\n\n[1] 16\n\n\nYou can also save one or more values into an object (think of this as a variable) and then do math with those objects:\n\nx &lt;- 10\n\ny &lt;- 5\n\nx*y\n\n[1] 50\n\n\nThere are several ways to store multiple values in one object, but the main method is using a function you’ve already used before:\n\nz &lt;- c(1, 2, 3, 4)\n\nz * x\n\n[1] 10 20 30 40\n\n\nThe c() function can be used to create vectors, which contain values for a single variable (here z). To access specific values within an object, you can use []:\n\nz[1]\n\n[1] 1\n\nz[1:3]\n\n[1] 1 2 3\n\nz[c(1,2,4)]\n\n[1] 1 2 4\n\n\nThere are also objects called data frames. These look more like your SPSS data files, or Excel files: big tables in which each row represents a case/person and each column represents a variable. The data we imported above is in a data frame. We can access several parts of the data frame using basic operations and functions:\n\n# retrieve the value in the first row, first column\ntempice[1,1]\n\n[1] 67.56\n\n# retrieve the first column\ntempice[,1]\n\n [1] 67.56 71.52 63.42 69.36 75.30 81.78 76.92 87.18 84.12 74.58 82.68 72.96\n\n# retrieve the second column by using its column name\ntempice$x2\n\n [1] 215 325 185 332 406 522 412 614 544 421 445 408\n\n# get some summary information about each column\nsummary(tempice)\n\n       x1              x2       \n Min.   :63.42   Min.   :185.0  \n 1st Qu.:70.98   1st Qu.:330.2  \n Median :74.94   Median :410.0  \n Mean   :75.61   Mean   :402.4  \n 3rd Qu.:82.00   3rd Qu.:464.2  \n Max.   :87.18   Max.   :614.0  \n\n\nThroughout this course, you will learn additional operations you can use in R. This class is not meant to be a complete introduction to the R language, so your knowledge of R will be somewhat haphazard by the end of this class."
  },
  {
    "objectID": "correlations.html#visualizing-bivariate-associations",
    "href": "correlations.html#visualizing-bivariate-associations",
    "title": "2  Correlations",
    "section": "2.4 Visualizing Bivariate Associations",
    "text": "2.4 Visualizing Bivariate Associations\nNow we can focus on the topic of this module: Correlation. We will start by producing a simple scatter plot to visualize the association between the two variables stored in tempice:\n\n# Create scatterplot of variables x1 and x2\nplot(x = tempice$x1, y = tempice$x2,\n     xlab = \"Temperature (F)\",\n     ylab = \"Ice cream sales ($)\")\n\n\n\n\nWe can also use the ggplot2 package to create a similar scatter plot:\n\nggplot(tempice, aes(x = x1, y = x2)) +\n  geom_point() +\n  labs(x = \"Temperature (F)\",\n       y = \"Ice cream sales ($)\")"
  },
  {
    "objectID": "correlations.html#calculating-pearsons-r-by-hand",
    "href": "correlations.html#calculating-pearsons-r-by-hand",
    "title": "2  Correlations",
    "section": "2.5 Calculating Pearson’s r ‘by hand’",
    "text": "2.5 Calculating Pearson’s r ‘by hand’\nNext, we will go through the different computational steps to calculate Pearson’s r.\n\nThe data\n\ntempice\n\n      x1  x2\n1  67.56 215\n2  71.52 325\n3  63.42 185\n4  69.36 332\n5  75.30 406\n6  81.78 522\n7  76.92 412\n8  87.18 614\n9  84.12 544\n10 74.58 421\n11 82.68 445\n12 72.96 408\n\n\n\n\nVariable x1 calculations\nFirst, we need to compute the mean, variance, and standard deviation for x1.\n\n# Mean:\nx1bar &lt;- (67.56 + 71.52 + 63.42 + 69.36 + 75.30 + 81.78 +\n            76.92 + 87.18 + 84.12 + 74.58 + 82.68 + 72.96) / 12\n\n# Mean (less by hand):\nx1bar_2 &lt;- sum(tempice$x1) / nrow(tempice)\n\n# The result is equivalent:\nx1bar\n\n[1] 75.615\n\nx1bar_2\n\n[1] 75.615\n\n# Variance:\ns2x1 &lt;- ((67.56 - x1bar) ^ 2 +\n           (71.52 - x1bar) ^ 2 +\n           (63.42 - x1bar) ^ 2 +\n           (69.36 - x1bar) ^ 2 +\n           (75.30 - x1bar) ^ 2 +\n           (81.78 - x1bar) ^ 2 +\n           (76.92 - x1bar) ^ 2 +\n           (87.18 - x1bar) ^ 2 +\n           (84.12 - x1bar) ^ 2 +\n           (74.58 - x1bar) ^ 2 +\n           (82.68 - x1bar) ^ 2 +\n           (72.96 - x1bar) ^ 2) / (12 - 1)\n\n# Variance (less by hand):\ns2x1_2 &lt;- sum((tempice$x1 - x1bar)^2) / (nrow(tempice) - 1)\n\n# Standard deviation:\nsx1_2 &lt;- sqrt(s2x1)\nsx1_2\n\n[1] 7.220069\n\n# Getting these things by doing even less by hand:\nx1bar &lt;- mean(tempice$x1)\ns2x1 &lt;- var(tempice$x1)\nsx1 &lt;- sd(tempice$x1)\nsx1\n\n[1] 7.220069\n\n\n\n\nVariable x2 calculations\nSecond, we need to compute the mean, variance, and standard deviation for x2. We will just use the built-in functions this time:\n\n# Same idea for variable x2:\nx2bar &lt;- mean(tempice$x2)\ns2x2 &lt;- var(tempice$x2)\nsx2 &lt;- sd(tempice$x2)\n\n\n\nSum of Cross-Products, Covarariance, and Correlation\nNext, we have to combine these components together to finnd thee sum of cross-products:\n\n# Compute the sum of cross-products:\nCP &lt;- (tempice$x1 - x1bar) * (tempice$x2 - x2bar)\nCP\n\n [1] 1509.64125  317.02125 2651.39625  440.45625   -1.12875  737.23125\n [7]   12.50625 2446.96125 1204.16625  -19.23375  300.85125  -14.82375\n\nsumCP &lt;- sum(CP)\nsumCP\n\n[1] 9585.045\n\n\nFinally, with help from the sample size, n, we can compute the sample covariance and (Pearson) correlation estimates:\n\n# Sample size\nn &lt;- nrow(tempice) \nn\n\n[1] 12\n\n# Covariance and correlation\ncovariance &lt;- sumCP/(n - 1)\ncovariance\n\n[1] 871.3677\n\ncorrelation &lt;- covariance/(sx1 * sx2)\n\n# Are ice cream sales and temperature correlated?\ncorrelation\n\n[1] 0.9575066"
  },
  {
    "objectID": "correlations.html#using-a-function-to-calculate-pearsons-r",
    "href": "correlations.html#using-a-function-to-calculate-pearsons-r",
    "title": "2  Correlations",
    "section": "2.6 Using a function to calculate Pearson’s r",
    "text": "2.6 Using a function to calculate Pearson’s r\nLuckily, R has some built-in functions that we can use to compute Pearson’s r:\n\ncov(tempice$x1, tempice$x2)\n\n[1] 871.3677\n\ncor(tempice$x1, tempice$x2)\n\n[1] 0.9575066\n\n\nAn even nicer option is to use a function that is part of the built-in stats package (this means you don’t have to install or load it yourself), which provides a confidence interval around the estimate:\n\ncor.test(tempice$x1, tempice$x2)\n\n\n    Pearson's product-moment correlation\n\ndata:  tempice$x1 and tempice$x2\nt = 10.499, df = 10, p-value = 1.016e-06\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8515370 0.9883148\nsample estimates:\n      cor \n0.9575066"
  },
  {
    "objectID": "correlations.html#issues-with-pearsons-r",
    "href": "correlations.html#issues-with-pearsons-r",
    "title": "2  Correlations",
    "section": "2.7 Issues with Pearson’s r",
    "text": "2.7 Issues with Pearson’s r\nTo see how misleading Pearson’s r can be when data do not meet its assumptions, we’ll look at a second data file.\nYou can download this data by right-clicking this link and selecting “Save Link As…” in the drop-down menu: data/tempicecurve.csv. Again: Make sure to save it in the folder you are using for this class.\nYou can import the data using a version of the code below, or using the point-and-click method described above.\n\ntempicecurve &lt;- import(file = \"data/tempicecurve.csv\")\n\nTo get an idea of the problem with these data, we can visualize them in another scatter plot:\n\nplot(tempicecurve$x1, tempicecurve$x2, pch=19,\n     xlab = \"Temperature (F)\",\n     ylab = \"Ice cream sales ($)\",\n     ylim = c(0,800))\n\n\n\n\nHow will the shape of the relationship between x1 and x2 affect the Pearson’s r correlation estimate?\n\ncor.test(tempicecurve$x1, tempicecurve$x2)\n\n\n    Pearson's product-moment correlation\n\ndata:  tempicecurve$x1 and tempicecurve$x2\nt = 0.0015808, df = 19, p-value = 0.9988\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.4313917  0.4319818\nsample estimates:\n         cor \n0.0003626502 \n\n\nThere might be an explanation for this kind of pattern. For example, it may be that there is a positive linear association up to a certain temperature after which the direction of the association changes because people don’t want to leave their house to buy Ice Cream anymore.\nTo visualize this hypothesis, we can use the group variable to change the color of point below and above a vague temperature cutoff range:\n\nplot(tempicecurve$x1,tempicecurve$x2,pch=19,\n     xlab = \"Temperature (F)\",\n     ylab = \"Ice cream sales ($)\",\n     ylim = c(0,800),\n     col = tempicecurve$group)\n\n\n\n\nWe can look at the correlation for each subset of data separately:\n\n# select only group = 1 (cooler to hot temps)\ntempicecurve1 &lt;- subset(tempicecurve, group == 1)\ncor.test(tempicecurve1$x1, tempicecurve1$x2)\n\n\n    Pearson's product-moment correlation\n\ndata:  tempicecurve1$x1 and tempicecurve1$x2\nt = 10.499, df = 10, p-value = 1.016e-06\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.8515370 0.9883148\nsample estimates:\n      cor \n0.9575066 \n\n# select only group = 2 (hot to hottest temps)\ntempicecurve2 &lt;- subset(tempicecurve, group == 2)\ncor.test(tempicecurve2$x1, tempicecurve2$x2)\n\n\n    Pearson's product-moment correlation\n\ndata:  tempicecurve2$x1 and tempicecurve2$x2\nt = -6.2426, df = 7, p-value = 0.0004272\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.9834759 -0.6604377\nsample estimates:\n      cor \n-0.920721 \n\n\nWhat do the separate correlation estimates tell us about the likely association between temperature and ice cream sales?\nWe can also use a non-parametric type of correlation, such as a distance correlation coefficient, to quantify the association between temperature and ice cream sales across the full range of temperatures. This coefficient ranges betwen 0 and 1 (so not -1 and 1), with 0 indicating no association between the variables and 1 indicating a perfect (but potentially non-linear) relationship between the variables. Thus, this value only tells you something about the strength of the association. After you compute this correlation, you need to use the scatter plot to describe the actual association.\n\ncor_test(tempicecurve, \"x1\", \"x2\", method = \"distance\")\n\nParameter1 | Parameter2 |    r |        95% CI | t(188) |      p\n----------------------------------------------------------------\nx1         |         x2 | 0.14 | [-0.31, 0.54] |   2.01 | 0.023*\n\nObservations: 21"
  },
  {
    "objectID": "correlations.html#alternatives-to-pearsons-r",
    "href": "correlations.html#alternatives-to-pearsons-r",
    "title": "2  Correlations",
    "section": "2.8 Alternatives to Pearson’s r",
    "text": "2.8 Alternatives to Pearson’s r\nIn the above example, we were able to split the data in half to appropriately estimate two Pearson’s r for two linear associations. But there are other alternative’s to Pearson’s r that help with other challenges.\n\nCorrelation Estimate for Data with Outliers\nWe need to import some more (fake) data, which you can download here: data/SATscores_outlier.csv\n\nSATscores_out &lt;- rio::import(\"data/SATscores_outlier.csv\")\n\nThis data frame contains two variables, verbal and quant, which reflect 11 participants’ verbal and quantitative SAT scores.\nBelow is code for visualizing the SATscores_out data, which reveals that there is an outlier.\n\nhist(SATscores_out$verbal, xlab = \"Verbal\", main = \"\")\n\n\n\nhist(SATscores_out$quant, xlab = \"Quant\", main = \"\")\n\n\n\n\nWe can compute the biweight and Winsorized correlation coefficients and compare those to the Pearson correlation coefficient:\n\n# Pearson\ncor.test(SATscores_out$verbal, SATscores_out$quant)\n\n\n    Pearson's product-moment correlation\n\ndata:  SATscores_out$verbal and SATscores_out$quant\nt = 3.8095, df = 9, p-value = 0.004157\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3513338 0.9417011\nsample estimates:\n      cor \n0.7856319 \n\n# biweight\ncor_test(SATscores_out, \"verbal\", \"quant\", method = \"biweight\")\n\nParameter1 | Parameter2 |    r |        95% CI | t(9) |     p\n-------------------------------------------------------------\nverbal     |      quant | 0.37 | [-0.29, 0.79] | 1.20 | 0.260\n\nObservations: 11\n\n# Winsorized\ncor_test(SATscores_out, \"verbal\", \"quant\", winsorize = TRUE)\n\nParameter1 | Parameter2 |    r |        95% CI | t(9) |     p\n-------------------------------------------------------------\nverbal     |      quant | 0.42 | [-0.24, 0.81] | 1.39 | 0.198\n\nObservations: 11\n\n\nHow does the estimate of the correlation change across methods?\n\n\nCorrelation Estimate for Non-normal Data\nEven without the outlier, the SAT scores distributions looked somewhat skewed. For this example, we will remove the outlier and focus solely on the non-normality of the two variables:\n\nSATscores &lt;- SATscores_out[1:10,]\n\nBelow is code to test if your variables are approximately Normally distributed. Remember, we’re testing the Null hypothesis that the data are similar to a Normal distribution. If the p-value is &lt; .05, we reject this Null hypothesis and have to conclude that the data are probably not normally distributed.\n\n# Shapiro Wilk test of normality.\nshapiro.test(SATscores$verbal)\n\n\n    Shapiro-Wilk normality test\n\ndata:  SATscores$verbal\nW = 0.82541, p-value = 0.02945\n\nshapiro.test(SATscores$quant)\n\n\n    Shapiro-Wilk normality test\n\ndata:  SATscores$quant\nW = 0.82188, p-value = 0.02671\n\n\nWe can compare Spearman’s \\(\\rho\\) (Rho) and Kendall’s \\(\\tau\\) (Tau) to Pearson’s correlation coefficient:\n\n# Pearson\ncor.test(SATscores$verbal, SATscores$quant)\n\n\n    Pearson's product-moment correlation\n\ndata:  SATscores$verbal and SATscores$quant\nt = 1.6325, df = 8, p-value = 0.1412\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.1893260  0.8591149\nsample estimates:\n      cor \n0.4998894 \n\n# Spearman (you can use cor.test or cor_test)\n# cor.test(SATscores$verbal, SATscores$quant, method = \"spearman\")\ncor_test(SATscores, \"verbal\", \"quant\", method = \"spearman\")\n\nParameter1 | Parameter2 |  rho |       95% CI |     S |      p\n--------------------------------------------------------------\nverbal     |      quant | 0.67 | [0.05, 0.92] | 54.00 | 0.033*\n\nObservations: 10\n\n# Kendall (you can use cor.test or cor_test)\n#cor.test(SATscores$verbal, SATscores$quant, method = \"kendall\")\ncor_test(SATscores, \"verbal\", \"quant\", method = \"kendall\")\n\nParameter1 | Parameter2 |  tau |       95% CI |    z |      p\n-------------------------------------------------------------\nverbal     |      quant | 0.51 | [0.04, 0.80] | 2.06 | 0.040*\n\nObservations: 10\n\n\nHow does the estimate of the correlation change across methods?\nWe can also compare the raw SAT data to the ranked SAT data to see that the correlation estimate is equivalent when using Kendall/Spearman, but that it is different when using Pearson.\nWe first create the rank-ordered variables:\n\nSATscores_rank &lt;- data.frame(verbal = rank(SATscores$verbal),\n                             quant = rank(SATscores$quant))\n\nNext, we look at the Pearson’s r when based on the raw or ranked data:\n\n# Camparing Pearson correlation coefficients \n# (now using the cor_test function)\ncor_test(SATscores, \"verbal\", \"quant\", method = \"pearson\")\n\nParameter1 | Parameter2 |    r |        95% CI | t(8) |     p\n-------------------------------------------------------------\nverbal     |      quant | 0.50 | [-0.19, 0.86] | 1.63 | 0.141\n\nObservations: 10\n\ncor_test(SATscores_rank, \"verbal\", \"quant\", method = \"pearson\")\n\nParameter1 | Parameter2 |    r |       95% CI | t(8) |      p\n-------------------------------------------------------------\nverbal     |      quant | 0.67 | [0.07, 0.91] | 2.57 | 0.033*\n\nObservations: 10\n\n\nNow compare those results to what happens when we use Spearman’s \\(rho\\) (Rho):\n\n# Comparing Spearman correlation coefficients\ncor_test(SATscores, \"verbal\", \"quant\", method = \"spearman\")\n\nParameter1 | Parameter2 |  rho |       95% CI |     S |      p\n--------------------------------------------------------------\nverbal     |      quant | 0.67 | [0.05, 0.92] | 54.00 | 0.033*\n\nObservations: 10\n\ncor_test(SATscores_rank, \"verbal\", \"quant\", method = \"spearman\")\n\nParameter1 | Parameter2 |  rho |       95% CI |     S |      p\n--------------------------------------------------------------\nverbal     |      quant | 0.67 | [0.05, 0.92] | 54.00 | 0.033*\n\nObservations: 10\n\n\n\n\nCorrelation Estimate for (Ordinal) Categorical Data\nFor this example, we will import some ordinal data on quality of life (QoL) and health, which you can download here: data/QoLHealth.csv\n\nQoLHealth &lt;- import(\"data/QoLHealth.csv\")\n\nThe variables are imported as strings, so we need to tell R what the order of the possible values is:\n\nQoLHealth$health &lt;- factor(QoLHealth$health, level = c(\"Poor\", \"Moderate\",\"Good\"), ordered = T)\nQoLHealth$QoL &lt;- factor(QoLHealth$QoL, level = c(\"Low\", \"Medium\", \"High\"), ordered = T)\n\nThe cross-table below shows the categorical nature of these variables, where each only takes on 3 values that may be ordinal but are not neccesarily equally spaced:\n\ntable(QoLHealth)\n\n          QoL\nhealth     Low Medium High\n  Poor      58     52    1\n  Moderate  26     58    3\n  Good       8     12    9\n\n\nWe can use the polychoric correlation coefficient for the ordinal QoL and Health data (here we use the correlation package):\n\ncor_test(QoLHealth, \"health\", \"QoL\", method = \"polychoric\")\n\nParameter1 | Parameter2 |  rho |       95% CI | t(225) |         p\n------------------------------------------------------------------\nhealth     |        QoL | 0.42 | [0.31, 0.52] |   6.94 | &lt; .001***\n\nObservations: 227\n\n\nThere is also an option in the psych package to compute the polychoric correlation coefficient, which uses the cross-table as input:\n\npolychoric(table(QoLHealth))\n\n[1] \"You seem to have a table, I will return just one correlation.\"\n\n\n$rho\n[1] 0.4198846\n\n$objective\n[1] 1.790876\n\n$tau.row\n       Poor    Moderate \n-0.02760955  1.13707578 \n\n$tau.col\n       Low     Medium \n-0.2396873  1.5781226 \n\n\nThe nice thing about the psych functions is that they also return the threshold estimates that represent the point on the underlying continuous distribution (e.g., the continuum of health from poor to good) where someone is likely to change their answer from one response category to the next."
  },
  {
    "objectID": "correlations.html#summary",
    "href": "correlations.html#summary",
    "title": "2  Correlations",
    "section": "2.9 Summary",
    "text": "2.9 Summary\nIn this R lab, you were introduced to a host of correlation coefficients, each of which are appropriate for different variable types and distributions. Next time you want to estimate the correlation between two variables, take a momnt to consider is Pearson’s r is the best choice or not."
  },
  {
    "objectID": "confirmatoryfactoranalysis.html#loading-r-packages",
    "href": "confirmatoryfactoranalysis.html#loading-r-packages",
    "title": "3  Confirmatory Factor Analysis",
    "section": "3.1 Loading R Packages",
    "text": "3.1 Loading R Packages\nRemember, you only need to install a package once. But if you want to use the functionality of a package, you will need to “load” the package into your environment. To do that for lavaan (and the semTools and psych packages, which we’ll also use in this lab), we use the library() function:\n\nlibrary(lavaan)\nlibrary(semTools)\nlibrary(psych)"
  },
  {
    "objectID": "confirmatoryfactoranalysis.html#loading-data-into-our-environment",
    "href": "confirmatoryfactoranalysis.html#loading-data-into-our-environment",
    "title": "3  Confirmatory Factor Analysis",
    "section": "3.2 Loading data into our environment",
    "text": "3.2 Loading data into our environment\nTypically, you will load your own data into your environment, like we did in the Correlations lab. However, you can also use datasets that are included with R packages. To access those datasets, you can use the data() function:\n\ndata(\"HolzingerSwineford1939\")\n\nIf you look at your environment tab, you should see a new data frame called HolzingerSwineford1939. We can take a look at the variables this dataframe using the describe() function that is part of the psych package:\n\ndescribe(HolzingerSwineford1939)\n\n        vars   n   mean     sd median trimmed    mad   min    max  range  skew\nid         1 301 176.55 105.94 163.00  176.78 140.85  1.00 351.00 350.00 -0.01\nsex        2 301   1.51   0.50   2.00    1.52   0.00  1.00   2.00   1.00 -0.06\nageyr      3 301  13.00   1.05  13.00   12.89   1.48 11.00  16.00   5.00  0.69\nagemo      4 301   5.38   3.45   5.00    5.32   4.45  0.00  11.00  11.00  0.09\nschool*    5 301   1.52   0.50   2.00    1.52   0.00  1.00   2.00   1.00 -0.07\ngrade      6 300   7.48   0.50   7.00    7.47   0.00  7.00   8.00   1.00  0.09\nx1         7 301   4.94   1.17   5.00    4.96   1.24  0.67   8.50   7.83 -0.25\nx2         8 301   6.09   1.18   6.00    6.02   1.11  2.25   9.25   7.00  0.47\nx3         9 301   2.25   1.13   2.12    2.20   1.30  0.25   4.50   4.25  0.38\nx4        10 301   3.06   1.16   3.00    3.02   0.99  0.00   6.33   6.33  0.27\nx5        11 301   4.34   1.29   4.50    4.40   1.48  1.00   7.00   6.00 -0.35\nx6        12 301   2.19   1.10   2.00    2.09   1.06  0.14   6.14   6.00  0.86\nx7        13 301   4.19   1.09   4.09    4.16   1.10  1.30   7.43   6.13  0.25\nx8        14 301   5.53   1.01   5.50    5.49   0.96  3.05  10.00   6.95  0.53\nx9        15 301   5.37   1.01   5.42    5.37   0.99  2.78   9.25   6.47  0.20\n        kurtosis   se\nid         -1.36 6.11\nsex        -2.00 0.03\nageyr       0.20 0.06\nagemo      -1.22 0.20\nschool*    -2.00 0.03\ngrade      -2.00 0.03\nx1          0.31 0.07\nx2          0.33 0.07\nx3         -0.91 0.07\nx4          0.08 0.07\nx5         -0.55 0.07\nx6          0.82 0.06\nx7         -0.31 0.06\nx8          1.17 0.06\nx9          0.29 0.06\n\n\nYou can also learn more about these built-in datasets by going to its help page:\n\n?HolzingerSwineford1939"
  },
  {
    "objectID": "confirmatoryfactoranalysis.html#cfa-step-1-model-specification",
    "href": "confirmatoryfactoranalysis.html#cfa-step-1-model-specification",
    "title": "3  Confirmatory Factor Analysis",
    "section": "3.3 CFA Step 1: Model Specification",
    "text": "3.3 CFA Step 1: Model Specification\nTo specify a model in lavaan, we have to write it out in lavaan syntax and assign it to an object (here called HSmodel). Basic syntax for a CFA follows this template:\nfactorname =~ indicator1 + indicator2 + indicator3\nIn our model, we have three factors (visual, textual, and speed) that each load onto three items (e.g., for visual: x1, x2, and x3). You don’t have to specify that factors are hypothesized to be correlated, lavaan does this automatically (scroll down for an example of a CFA in which we specify that the factors should not be correlated).\n\n#CFA model specification\nHSmodel &lt;- \"visual  =~ x1 + x2 + x3\n            textual =~ x4 + x5 + x6\n            speed   =~ x7 + x8 + x9\""
  },
  {
    "objectID": "confirmatoryfactoranalysis.html#cfa-step-2-model-estimation",
    "href": "confirmatoryfactoranalysis.html#cfa-step-2-model-estimation",
    "title": "3  Confirmatory Factor Analysis",
    "section": "3.4 CFA Step 2: Model Estimation",
    "text": "3.4 CFA Step 2: Model Estimation\nNext, we need to estimate the model, using our data. In our lecture, we went over the four phases of estimation, but in R, model estimation simplifies to using the cfa() function with our model syntax and data arguments:\n\nfit1 &lt;- cfa(model = HSmodel, \n            data = HolzingerSwineford1939)"
  },
  {
    "objectID": "confirmatoryfactoranalysis.html#cfa-step-3-interpreting-model-fit-and-parameter-estimates",
    "href": "confirmatoryfactoranalysis.html#cfa-step-3-interpreting-model-fit-and-parameter-estimates",
    "title": "3  Confirmatory Factor Analysis",
    "section": "3.5 CFA Step 3: Interpreting Model Fit and Parameter Estimates",
    "text": "3.5 CFA Step 3: Interpreting Model Fit and Parameter Estimates\nThere are several ways of extracting the model fit and parameter estimates from our fitted lavaan model (called fit1). The most typical way to look at this output is by using the summary() function. Within this function, we can ask for some extra output (fit measures, standardized estimate, R-squares):\n\nsummary(fit1, \n        fit.measures = TRUE, \n        standardized = TRUE, \n        rsquare = TRUE)\n\nlavaan 0.6.17 ended normally after 35 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        21\n\n  Number of observations                           301\n\nModel Test User Model:\n                                                      \n  Test statistic                                85.306\n  Degrees of freedom                                24\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               918.852\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.931\n  Tucker-Lewis Index (TLI)                       0.896\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -3737.745\n  Loglikelihood unrestricted model (H1)      -3695.092\n                                                      \n  Akaike (AIC)                                7517.490\n  Bayesian (BIC)                              7595.339\n  Sample-size adjusted Bayesian (SABIC)       7528.739\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.092\n  90 Percent confidence interval - lower         0.071\n  90 Percent confidence interval - upper         0.114\n  P-value H_0: RMSEA &lt;= 0.050                    0.001\n  P-value H_0: RMSEA &gt;= 0.080                    0.840\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.065\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  visual =~                                                             \n    x1                1.000                               0.900    0.772\n    x2                0.554    0.100    5.554    0.000    0.498    0.424\n    x3                0.729    0.109    6.685    0.000    0.656    0.581\n  textual =~                                                            \n    x4                1.000                               0.990    0.852\n    x5                1.113    0.065   17.014    0.000    1.102    0.855\n    x6                0.926    0.055   16.703    0.000    0.917    0.838\n  speed =~                                                              \n    x7                1.000                               0.619    0.570\n    x8                1.180    0.165    7.152    0.000    0.731    0.723\n    x9                1.082    0.151    7.155    0.000    0.670    0.665\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  visual ~~                                                             \n    textual           0.408    0.074    5.552    0.000    0.459    0.459\n    speed             0.262    0.056    4.660    0.000    0.471    0.471\n  textual ~~                                                            \n    speed             0.173    0.049    3.518    0.000    0.283    0.283\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .x1                0.549    0.114    4.833    0.000    0.549    0.404\n   .x2                1.134    0.102   11.146    0.000    1.134    0.821\n   .x3                0.844    0.091    9.317    0.000    0.844    0.662\n   .x4                0.371    0.048    7.779    0.000    0.371    0.275\n   .x5                0.446    0.058    7.642    0.000    0.446    0.269\n   .x6                0.356    0.043    8.277    0.000    0.356    0.298\n   .x7                0.799    0.081    9.823    0.000    0.799    0.676\n   .x8                0.488    0.074    6.573    0.000    0.488    0.477\n   .x9                0.566    0.071    8.003    0.000    0.566    0.558\n    visual            0.809    0.145    5.564    0.000    1.000    1.000\n    textual           0.979    0.112    8.737    0.000    1.000    1.000\n    speed             0.384    0.086    4.451    0.000    1.000    1.000\n\nR-Square:\n                   Estimate\n    x1                0.596\n    x2                0.179\n    x3                0.338\n    x4                0.725\n    x5                0.731\n    x6                0.702\n    x7                0.324\n    x8                0.523\n    x9                0.442\n\n\n\nModel Fit\nThe output above is great, but it can be a lot. To look at just the fit indices, you can also use fitMeasures(). This function will return a ton of fit indices if you do not include the fit.measures argument. Here, we select the main indices that we’re intereted in:\n\nfitMeasures(fit1, \n            fit.measures = c(\"chisq\", \"df\", \"pvalue\",\n                             \"cfi\", \"rmsea\", \"rmsea.ci.lower\", \n                             \"rmsea.ci.upper\", \"srmr\"))\n\n         chisq             df         pvalue            cfi          rmsea \n        85.306         24.000          0.000          0.931          0.092 \nrmsea.ci.lower rmsea.ci.upper           srmr \n         0.071          0.114          0.065 \n\n\nWe can include output = \"text\" to make the output look a little bit nicer (more like the summary output above):\n\nfitMeasures(fit1, \n            fit.measures = c(\"chisq\", \"df\", \"pvalue\",\n                             \"cfi\", \"rmsea\", \"rmsea.ci.lower\", \n                             \"rmsea.ci.upper\", \"srmr\"),\n            output = \"text\")\n\n\nModel Test User Model:\n\n  Test statistic                                85.306\n  Degrees of freedom                                24\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.931\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.092\n  Confidence interval - lower                    0.071\n  Confidence interval - upper                    0.114\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.065\n\n\nBased on all fit indices, the fit of this CFA is poor. First, the Chi-square statistic is signifiicant, indicating poor fit. Sometimes, with larger sample sizes, a significant Chi-square simply means that there are a lot of small, trivial misspecifications. However, if we look at the CFI, TLI, RMSEA, and SRMR values and compare them to their suggested cutoff values (.95, .95, .06, .08), they also indicate that the model fits the data poorly.\n\n\nParameter Estimates\nThere is also a function we can use to extract just the standardized estimates, standardizedSolution(). Within this function, we can specify that some of the output (z-statistics and p-values) are left out. Typically, significance of parameter estimates is evaluated using the unstandardized solution (which we saw above with the summary() function), so we should not focus on significance of the standardized estimates. Again, we can include output = \"text\" to get output that is easier to read:\n\nstandardizedSolution(fit1, \n                     zstat = FALSE, pvalue = FALSE,\n                     output = \"text\")\n\n\nLatent Variables:\n                    est.std  Std.Err ci.lower ci.upper\n  visual =~                                           \n    x1                0.772    0.055    0.664    0.880\n    x2                0.424    0.060    0.307    0.540\n    x3                0.581    0.055    0.473    0.689\n  textual =~                                          \n    x4                0.852    0.023    0.807    0.896\n    x5                0.855    0.022    0.811    0.899\n    x6                0.838    0.023    0.792    0.884\n  speed =~                                            \n    x7                0.570    0.053    0.465    0.674\n    x8                0.723    0.051    0.624    0.822\n    x9                0.665    0.051    0.565    0.765\n\nCovariances:\n                    est.std  Std.Err ci.lower ci.upper\n  visual ~~                                           \n    textual           0.459    0.064    0.334    0.584\n    speed             0.471    0.073    0.328    0.613\n  textual ~~                                          \n    speed             0.283    0.069    0.148    0.418\n\nVariances:\n                    est.std  Std.Err ci.lower ci.upper\n   .x1                0.404    0.085    0.238    0.571\n   .x2                0.821    0.051    0.722    0.920\n   .x3                0.662    0.064    0.537    0.788\n   .x4                0.275    0.038    0.200    0.350\n   .x5                0.269    0.038    0.194    0.344\n   .x6                0.298    0.039    0.221    0.374\n   .x7                0.676    0.061    0.557    0.794\n   .x8                0.477    0.073    0.334    0.620\n   .x9                0.558    0.068    0.425    0.691\n    visual            1.000             1.000    1.000\n    textual           1.000             1.000    1.000\n    speed             1.000             1.000    1.000"
  },
  {
    "objectID": "confirmatoryfactoranalysis.html#cfa-step-4-making-model-adjustments",
    "href": "confirmatoryfactoranalysis.html#cfa-step-4-making-model-adjustments",
    "title": "3  Confirmatory Factor Analysis",
    "section": "3.6 CFA Step 4: Making Model Adjustments",
    "text": "3.6 CFA Step 4: Making Model Adjustments\nOur model did not fit our data very well, so we may want to make some adjustments to our model. Remember, there are always ways to make a model fit better (adding parameters), but if they are not informed and justified by theory then we end up with a model that will only fit our data (like the outfits made specifically for Taylor Swift during her Eras tour) and will not generalize to new samples (which is bad!).\nWe can use a function to help us identify the parameters that, when added, will result in the largest improvements in model fit (in terms of a reduction in the model Chi-square statistic), the modindices() function (which stands for modification indices). In the code below, we ask that only parameters with relatively large modification indices (10 or above) are returned, and we ask that the output is sorted from largest improvement to smallest improvement.\n\nmodindices(fit1, minimum.value = 10, sort = TRUE)\n\n      lhs op rhs     mi    epc sepc.lv sepc.all sepc.nox\n30 visual =~  x9 36.411  0.577   0.519    0.515    0.515\n76     x7 ~~  x8 34.145  0.536   0.536    0.859    0.859\n28 visual =~  x7 18.631 -0.422  -0.380   -0.349   -0.349\n78     x8 ~~  x9 14.946 -0.423  -0.423   -0.805   -0.805\n\n\n\nModel Re-Specification\nWe can use the information from the modification indices to re-specify our model. The largest index is for adding a factor loading that goes from the visual factor to x9 (mi = 36.41), which is “Speeded discrimination straight and curved capitals”. From the description of x9, we can see that this test does involve visual ability as well, so I feel that we can justify this modification. This means that the item x9 now loads onto two factors. The second loading is also called a cross loading. By adding this cross loadings, we’re making the interpretation of the visual and speed factors more complex.\n\n#Reanalysis\nHSmodel2 &lt;- \"visual  =~ x1 + x2 + x3 + x9\n             textual =~ x4 + x5 + x6\n             speed   =~ x7 + x8 + x9\"\n\n\n\nModel Estimation\nAgain, model estimation is straightforward:\n\nfit2 &lt;- cfa(model = HSmodel2, \n            data = HolzingerSwineford1939)\n\n\n\nModel Fit and Parameter Interpretation\nWe can look at the model fit and parameter estimates of this new model.\n\nsummary(fit2, \n        fit.measures = TRUE, \n        standardized = TRUE, \n        rsquare = TRUE)\n\nlavaan 0.6.17 ended normally after 34 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                        22\n\n  Number of observations                           301\n\nModel Test User Model:\n                                                      \n  Test statistic                                52.382\n  Degrees of freedom                                23\n  P-value (Chi-square)                           0.000\n\nModel Test Baseline Model:\n\n  Test statistic                               918.852\n  Degrees of freedom                                36\n  P-value                                        0.000\n\nUser Model versus Baseline Model:\n\n  Comparative Fit Index (CFI)                    0.967\n  Tucker-Lewis Index (TLI)                       0.948\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -3721.283\n  Loglikelihood unrestricted model (H1)      -3695.092\n                                                      \n  Akaike (AIC)                                7486.566\n  Bayesian (BIC)                              7568.123\n  Sample-size adjusted Bayesian (SABIC)       7498.351\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.065\n  90 Percent confidence interval - lower         0.042\n  90 Percent confidence interval - upper         0.089\n  P-value H_0: RMSEA &lt;= 0.050                    0.133\n  P-value H_0: RMSEA &gt;= 0.080                    0.158\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.045\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nLatent Variables:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  visual =~                                                             \n    x1                1.000                               0.885    0.759\n    x2                0.578    0.098    5.918    0.000    0.511    0.435\n    x3                0.754    0.103    7.291    0.000    0.667    0.590\n    x9                0.437    0.081    5.367    0.000    0.387    0.384\n  textual =~                                                            \n    x4                1.000                               0.989    0.851\n    x5                1.115    0.066   17.016    0.000    1.103    0.856\n    x6                0.926    0.056   16.685    0.000    0.916    0.838\n  speed =~                                                              \n    x7                1.000                               0.666    0.612\n    x8                1.207    0.185    6.540    0.000    0.804    0.795\n    x9                0.675    0.112    6.037    0.000    0.450    0.447\n\nCovariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n  visual ~~                                                             \n    textual           0.396    0.072    5.506    0.000    0.453    0.453\n    speed             0.177    0.055    3.239    0.001    0.301    0.301\n  textual ~~                                                            \n    speed             0.136    0.051    2.675    0.007    0.206    0.206\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n   .x1                0.576    0.100    5.731    0.000    0.576    0.424\n   .x2                1.120    0.100   11.153    0.000    1.120    0.811\n   .x3                0.830    0.087    9.515    0.000    0.830    0.651\n   .x9                0.558    0.060    9.336    0.000    0.558    0.550\n   .x4                0.373    0.048    7.800    0.000    0.373    0.276\n   .x5                0.444    0.058    7.602    0.000    0.444    0.267\n   .x6                0.357    0.043    8.285    0.000    0.357    0.298\n   .x7                0.740    0.086    8.595    0.000    0.740    0.625\n   .x8                0.375    0.094    3.973    0.000    0.375    0.367\n    visual            0.783    0.134    5.842    0.000    1.000    1.000\n    textual           0.978    0.112    8.728    0.000    1.000    1.000\n    speed             0.444    0.097    4.567    0.000    1.000    1.000\n\nR-Square:\n                   Estimate\n    x1                0.576\n    x2                0.189\n    x3                0.349\n    x9                0.450\n    x4                0.724\n    x5                0.733\n    x6                0.702\n    x7                0.375\n    x8                0.633\n\n\nBut how do we know if this model is better (in terms of fitting our data) than the original CFA?"
  },
  {
    "objectID": "confirmatoryfactoranalysis.html#comparing-multiple-models-to-each-other",
    "href": "confirmatoryfactoranalysis.html#comparing-multiple-models-to-each-other",
    "title": "3  Confirmatory Factor Analysis",
    "section": "3.7 Comparing Multiple Models to Each Other",
    "text": "3.7 Comparing Multiple Models to Each Other\nTo compare the fit of the two models, we can use the compareFit() function:\n\ncomp_fit1_fit2 &lt;- compareFit(fit1, fit2)\nsummary(comp_fit1_fit2)\n\n################### Nested Model Comparison #########################\n\nChi-Squared Difference Test\n\n     Df    AIC    BIC  Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)    \nfit2 23 7486.6 7568.1 52.382                                          \nfit1 24 7517.5 7595.3 85.305     32.923 0.32567       1  9.586e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n####################### Model Fit Indices ###########################\n       chisq df pvalue rmsea   cfi   tli  srmr       aic       bic\nfit2 52.382† 23   .000 .065† .967† .948† .045† 7486.566† 7568.123†\nfit1 85.306  24   .000 .092  .931  .896  .065  7517.490  7595.339 \n\n################## Differences in Fit Indices #######################\n            df rmsea    cfi    tli srmr    aic    bic\nfit1 - fit2  1 0.027 -0.036 -0.052 0.02 30.923 27.216\n\n\nThe output of this function includes a comparison of fit based on the model Chi-square: the Chi-square difference test. If this test is significant, then it means that the model with fewer estimated parameters (here fit1) fits the data significantly worse than the model with more estimated parameters (here fit2), so we should select the model with more parameters (fit2). If this test is not significant, then it means that the additional parameters of fit2 did not improve fit enough to result in a better model fit than that of fit1, which means that we should stick with the simpler model (fit1). What do the resuts above tell us?\nThe output also includes comparisons of the relative fit indices (AIC and BIC). Remember: lower values indicate better fit. Finally, the output also includes comparisons of other fit indices (e.g., CFI, RMSEA). These are sometimes also used to compare the fit of several models. However, there are no clear, universal guidelines on how different these indices need to be before they indicate an improvement/worsening in model fit."
  },
  {
    "objectID": "confirmatoryfactoranalysis.html#how-to-specify-different-types-of-cfas",
    "href": "confirmatoryfactoranalysis.html#how-to-specify-different-types-of-cfas",
    "title": "3  Confirmatory Factor Analysis",
    "section": "3.8 How to specify different types of CFAs",
    "text": "3.8 How to specify different types of CFAs\nHere is an example of how to specify a hierarchical CFA, where the three factors are indicator of a higher-order ability factor:\n\nHSmodel3 &lt;- \"visual =~ x1 + x2 + x3\n            textual =~ x4 + x5 + x6\n            speed   =~ x7 + x8 + x9\n\n            ability =~ visual + textual + speed\"\n\nHere is an example of a CFA in which the factors are specified to be uncorrelated with each other:\n\nHSmodel4 &lt;- \"visual =~ x1 + x2 + x3\n            textual =~ x4 + x5 + x6\n            speed   =~ x7 + x8 + x9\n\n            visual ~~ 0*textual\n            visual ~~ 0*speed\n            textual ~~ 0*speed\"\n\nFactor correlations can be fixed to 0 (i.e., removed from the CFA) using the following template:\nfactorname1 ~~ 0*factorname2\nHere is an example of a CFA in which the factors are specified to be correlated but the correlations are constrained to be equal (i.e., their parameter estimate is going to be the exact same value):\n\nHSmodel5 &lt;- \"visual =~ x1 + x2 + x3\n            textual =~ x4 + x5 + x6\n            speed   =~ x7 + x8 + x9\n\n            visual ~~ a*textual\n            visual ~~ a*speed\n            textual ~~ a* speed\"\n\nTo constrain parameters to be equal, we can give them the same label (this is different from the factorname that we use to specify/name latent factors). The general format for these equality constraints is:\nfactorname1 ~~ label*factorname2\nOr for constraining factor loadings to be equivalent:\nfactorname1 =~ label*x1 + label*x2 + label*x3\nYour label can be any text string (e.g., a, b, eq), but remember to use different labels for different equality constraints. So, if you want to constrain your loadings and your factor correlations, use the label a for the loadings and b for the correlations."
  },
  {
    "objectID": "confirmatoryfactoranalysis.html#summary",
    "href": "confirmatoryfactoranalysis.html#summary",
    "title": "3  Confirmatory Factor Analysis",
    "section": "3.9 Summary",
    "text": "3.9 Summary\nIn this R lab, you learned how to specify, estimate, evaluate and interpret CFAs. In addition, you learned how to re-specify a CFA and compare the fit across several models to select the best-fitting model. Finally, you were introduced to some examples of alternative CFA configurations, such as the higher-order CFA."
  },
  {
    "objectID": "exploratoryfactoranalysis.html#loading-r-packages",
    "href": "exploratoryfactoranalysis.html#loading-r-packages",
    "title": "4  Exploratory Factor Analysis",
    "section": "4.1 Loading R Packages",
    "text": "4.1 Loading R Packages\nRemember, you only need to install a package once. But if you want to use the functionality of a package, you will need to “load” the package into your environment. To do that for lavaan (and the psych and GPArotation packages, which we’ll also use in this lab), we use the library() function:\n\nlibrary(lavaan)\nlibrary(psych)\nlibrary(GPArotation)"
  },
  {
    "objectID": "exploratoryfactoranalysis.html#loading-data-into-our-environment",
    "href": "exploratoryfactoranalysis.html#loading-data-into-our-environment",
    "title": "4  Exploratory Factor Analysis",
    "section": "4.2 Loading data into our environment",
    "text": "4.2 Loading data into our environment\nWe’re using the same dataset as we used in the CFA R Lab, so we can use the data() function like we did before:\n\ndata(\"HolzingerSwineford1939\")\n\nRemember that for the CFA analysis, we did not have to remove any variables from the data frame, because lavaan extracted the relevant variables automatically. With EFA, using the psych package, we have to do that extraction ourselves. We can do that as follows:\n\n# print the variable names of the full data frame \n# and locate the relevant variables\ncolnames(HolzingerSwineford1939)\n\n [1] \"id\"     \"sex\"    \"ageyr\"  \"agemo\"  \"school\" \"grade\"  \"x1\"     \"x2\"    \n [9] \"x3\"     \"x4\"     \"x5\"     \"x6\"     \"x7\"     \"x8\"     \"x9\"    \n\n# use the [,] operator to select only the relevant \n# columns/variables (here in column 7 to 15)\nHSdata &lt;- HolzingerSwineford1939[,7:15]\n\nThere are many other ways of selecting variables from a larger data frame, and if you have a different method that you like better, feel free to use it!"
  },
  {
    "objectID": "exploratoryfactoranalysis.html#efa-step-1-how-many-factors-should-i-extract",
    "href": "exploratoryfactoranalysis.html#efa-step-1-how-many-factors-should-i-extract",
    "title": "4  Exploratory Factor Analysis",
    "section": "4.3 EFA Step 1: How many factors should I extract?",
    "text": "4.3 EFA Step 1: How many factors should I extract?\nIn the first step of the EFA, we will use parallel analysis to see what the algorithm identifies as the optimal number of factors to extract from the data. This algorithm generates random correlation matrices, and when doing so, it may return an error message because something went wrong with those random matrices. If this happens, you can simply re-run the fa.parallel() function and the error should disappear.\nThe code below will return a parallel analysis for the Holzinger Swineford data using the factor analysis method and based on 50 random correlation matrices. You can increase that number to 100 or 1000 if you want to be more certain of the results, but note that that will take longer to run.\n\nfa.parallel(HSdata, fa = \"fa\", n.iter = 50)\n\n\n\n\nParallel analysis suggests that the number of factors =  3  and the number of components =  NA \n\n\nBased on the plot, how many factors should we extract?"
  },
  {
    "objectID": "exploratoryfactoranalysis.html#efa-step-2-factor-extraction-and-rotation",
    "href": "exploratoryfactoranalysis.html#efa-step-2-factor-extraction-and-rotation",
    "title": "4  Exploratory Factor Analysis",
    "section": "4.4 EFA Step 2: Factor Extraction and Rotation",
    "text": "4.4 EFA Step 2: Factor Extraction and Rotation\nIn the first cycle of the EFA process, we will follow the parallel analysis results and estimate a 3-factor EFA. To estimate the parameters and rotate those results to be more interpretable, we just need to use one function:\n\nefa_3f &lt;- fa(HSdata, nfactors = 3, \n             fm = \"minres\", \n             rotate = \"oblimin\")\n\nTechnically, you don’t even need to include fm = \"minres\", rotate = \"oblimin\", but I wanted to show you what arguments you need to use if you want to change the default estimation method (here “minres”) or if you want to change the default rotation method (here “oblimin”)."
  },
  {
    "objectID": "exploratoryfactoranalysis.html#efa-step-3-interpreting-the-efa-estimates",
    "href": "exploratoryfactoranalysis.html#efa-step-3-interpreting-the-efa-estimates",
    "title": "4  Exploratory Factor Analysis",
    "section": "4.5 EFA Step 3: Interpreting the EFA estimates",
    "text": "4.5 EFA Step 3: Interpreting the EFA estimates\n\nCommmunalities\n\nround(efa_3f$communalities, \n      digits = 3)\n\n   x1    x2    x3    x4    x5    x6    x7    x8    x9 \n0.477 0.255 0.453 0.728 0.754 0.691 0.519 0.520 0.460 \n\n\nMost communalities are between .4 and .6 (one even above .6), indicating that the factors are able to account for a good chunk of the variability in the item responses. One exception is x2, which has a communality of .255. Overall, these values look acceptable.\n\n\nFactor Loadings\nIt can be helpful to hide low factor loadings from your output to see if the factor extraction and rotation has resulted in a simple structure. We can do that by including cutoff = .3 in the print() function:\n\nprint(efa_3f$loadings, \n      cutoff = .3)\n\n\nLoadings:\n   MR1    MR3    MR2   \nx1         0.592       \nx2         0.509       \nx3         0.686       \nx4  0.846              \nx5  0.886              \nx6  0.805              \nx7                0.737\nx8                0.686\nx9         0.382  0.456\n\n                 MR1   MR3   MR2\nSS loadings    2.197 1.275 1.239\nProportion Var 0.244 0.142 0.138\nCumulative Var 0.244 0.386 0.523\n\n\nThe factor loadings appear to follow a pretty clear, simple structure. The exception is x9, which has a factor loading &gt; .3 on two factors.\nThis output also includes information about the variance in the items that is explained by each factor. SS loadings refers to the sum of the squared loadings (i.e., the factor’s Eigenvalue). The columns (even in the loadings table) are sorted from the highest Eigenvalue to the lowest. That’s why the order here is MR1, MR3, and then MR2 (and MR refers to the estimation method, minres). The second row shows the variance that is accounted for by each factor, and the bottom row shows the cumulative variance accounted for by all factors. Here, the three factors explain 52.3% of the variance in the items.\nNote: if you want to see all the factor loading estimates, you need to set the cuttof at the lowest possible value for factorloadings (-1):\n\nprint(efa_3f$loadings, \n      cutoff = -1)\n\n\nLoadings:\n   MR1    MR3    MR2   \nx1  0.196  0.592  0.031\nx2  0.043  0.509 -0.122\nx3 -0.062  0.686  0.019\nx4  0.846  0.016  0.008\nx5  0.886 -0.065  0.007\nx6  0.805  0.080 -0.013\nx7  0.044 -0.152  0.737\nx8 -0.034  0.125  0.686\nx9  0.032  0.382  0.456\n\n                 MR1   MR3   MR2\nSS loadings    2.197 1.275 1.239\nProportion Var 0.244 0.142 0.138\nCumulative Var 0.244 0.386 0.523\n\n\n\n\nFactor Correlations\nFinally, we can look at the correlations between the factors:\n\nround(efa_3f$Phi, \n      digits = 3)\n\n      MR1   MR3   MR2\nMR1 1.000 0.323 0.213\nMR3 0.323 1.000 0.261\nMR2 0.213 0.261 1.000\n\n\nExtremely large correlations between factors may be an indication of overextraction; the two factors could be combined into one factor. In this case, the correlations between the factors are small to moderate, indicating that they are tapping into distinct but correlated subconstructs."
  },
  {
    "objectID": "exploratoryfactoranalysis.html#efa-step-4-comparing-to-other-factor-solutions",
    "href": "exploratoryfactoranalysis.html#efa-step-4-comparing-to-other-factor-solutions",
    "title": "4  Exploratory Factor Analysis",
    "section": "4.6 EFA Step 4: Comparing to other factor solutions",
    "text": "4.6 EFA Step 4: Comparing to other factor solutions\nTo understand if the three-factor model makes the most sense, it is typical to also estimate an EFA with one factor less and one factor more to see if those analyses result in more clearly interpretable results. Let’s start by estimating a two-factor EFA.\n\nTwo-Factor EFA\n\nefa_2f &lt;- fa(HSdata, nfactors = 2, \n             fm = \"minres\", \n             rotate = \"oblimin\")\n\n\n\nInterpreting the results of the Two-Factor EFA\n\nround(efa_2f$communalities, \n      digits = 3)\n\n   x1    x2    x3    x4    x5    x6    x7    x8    x9 \n0.341 0.100 0.223 0.728 0.708 0.705 0.179 0.381 0.545 \n\n\nMany of the communalities are low, indicating that this factor solution does not do a good job of accounting for variability in the items.\n\nprint(efa_2f$loadings, \n      cutoff = .3)\n\n\nLoadings:\n   MR1    MR2   \nx1         0.430\nx2              \nx3         0.449\nx4  0.851       \nx5  0.854       \nx6  0.828       \nx7         0.434\nx8         0.640\nx9         0.736\n\n                 MR1   MR2\nSS loadings    2.244 1.588\nProportion Var 0.249 0.176\nCumulative Var 0.249 0.426\n\n\nAlthough there are no cross-loadings, one item (x2) doesn’t have a loading &gt; .3 on either of the factors! These two factors cannot capture the variance in x2 that is common with the other items.\n\nround(efa_2f$Phi, \n      digits = 3)\n\n     MR1  MR2\nMR1 1.00 0.34\nMR2 0.34 1.00\n\n\nThe factor correlation does not indicate any issues.\n\n\nFour-Factor EFA\n\nefa_4f &lt;- fa(HSdata, nfactors = 4, \n             fm = \"minres\", \n             rotate = \"oblimin\")\n\n\nInterpreting the results of the Four-Factor EFA\n\nround(efa_4f$communalities, \n      digits = 3)\n\n   x1    x2    x3    x4    x5    x6    x7    x8    x9 \n0.454 0.230 0.554 0.740 0.787 0.687 0.995 0.424 0.568 \n\n\nCompared to the three-factor EFA, the communalities have not changed a lot, except for the communality of x7, which is now a whopping .995. Such a high communality indicates that there is a factor (or combination of factors) that can account for almost all variability in x7. Although this may sound good, it may stand in the way of our goal of dimension reduction (as we’ll see next).\n\nprint(efa_4f$loadings, \n      cutoff = .3)\n\n\nLoadings:\n   MR1    MR2    MR3    MR4   \nx1                0.453       \nx2                0.397       \nx3                0.735       \nx4  0.850                     \nx5  0.887                     \nx6  0.804                     \nx7         0.986              \nx8                       0.499\nx9                       0.674\n\n                 MR1   MR2   MR3   MR4\nSS loadings    2.216 1.089 0.954 0.787\nProportion Var 0.246 0.121 0.106 0.087\nCumulative Var 0.246 0.367 0.473 0.561\n\n\nWhen asked to estimate four factors, the EFA algorithm resulted in a factor that only represents one item (x7). This one-to-one association gets in the way of our goal of dimension reduction, and is an indication that this factor solution is not appropriate.\n\nround(efa_4f$Phi, digits = 3)\n\n      MR1   MR2   MR3   MR4\nMR1 1.000 0.122 0.250 0.286\nMR2 0.122 1.000 0.046 0.417\nMR3 0.250 0.046 1.000 0.436\nMR4 0.286 0.417 0.436 1.000\n\n\nFactor correlations indicate that there is no extremely strong correlation (\\(r = .417\\)) between the factor that represents x7 and the factor that represents x8 and x9 (these three items are hypothesized to measure one subconstruct: speed). This indicates that, although these three items share some common variance, they also tap into distinct sub-subconstructs that may need to be explored further."
  },
  {
    "objectID": "exploratoryfactoranalysis.html#some-final-conclusions",
    "href": "exploratoryfactoranalysis.html#some-final-conclusions",
    "title": "4  Exploratory Factor Analysis",
    "section": "4.7 Some Final Conclusions",
    "text": "4.7 Some Final Conclusions\nFor this sample, a three-factor solution appeared to best balance dimension reduction and representing the associations among the observed variables. However, the results did indicate that there may be an issue with x2 (low communality) and x9 (cross loading). In addition, the four-factor EFA seemed to indicate that the three items measuring speed are not as related as we’d hoped they’d be. A second sample could indicate whether these findings were due to sampling variability or whether they reflect true issues that need to be resolved."
  },
  {
    "objectID": "exploratoryfactoranalysis.html#summary",
    "href": "exploratoryfactoranalysis.html#summary",
    "title": "4  Exploratory Factor Analysis",
    "section": "4.8 Summary",
    "text": "4.8 Summary\nIn this R lab, you learned how to specify, estimate, evaluate and interpret EFAs. You also learned how to evaluate different sources of information about the appropriateness of the EFA solutions."
  },
  {
    "objectID": "reliability.html#installing-and-loading-the-r-packages",
    "href": "reliability.html#installing-and-loading-the-r-packages",
    "title": "5  Reliability",
    "section": "5.1 Installing and Loading the R packages",
    "text": "5.1 Installing and Loading the R packages\nIn this lab we will be using a new package: MBESS (Methods for the Behavioral, Educational, and Social Sciences), which includes a function that calculates coefficient omega and (more importantly) its confidence interval. Although other packages can also estimate coefficient omega, they often do not provide a confidence interval.\n\ninstall.packages(\"MBESS\")\n\n\n\n\n\n\n\nTip\n\n\n\nIf you experience issues installing this package on macOS, you likely need to install a few additional tools. Go to this page to download and install those tools: Compile Tools for macOS.\n\n\nIn this lab, we will also use the psych package, which we have already installed in earlier labs. So, we can simply get those packages from the R package library:\n\nlibrary(psych)\nlibrary(MBESS)"
  },
  {
    "objectID": "reliability.html#loading-data-into-our-environment",
    "href": "reliability.html#loading-data-into-our-environment",
    "title": "5  Reliability",
    "section": "5.2 Loading data into our environment",
    "text": "5.2 Loading data into our environment\nFor this lab, we will use a dataset that is included in the psych package, so we can use the data() function like we did before:\n\ndata(\"attitude\")\n\nThese data come from a survey of clerical employees of a large financial organization. Each variable represents a rating (on the percentage scale) of how well the company performs on that item’s topic (e.g., complaints).\n\ndescribe(attitude)\n\n           vars  n  mean    sd median trimmed   mad min max range  skew\nrating        1 30 64.63 12.17   65.5   65.21 10.38  40  85    45 -0.36\ncomplaints    2 30 66.60 13.31   65.0   67.08 14.83  37  90    53 -0.22\nprivileges    3 30 53.13 12.24   51.5   52.75 10.38  30  83    53  0.38\nlearning      4 30 56.37 11.74   56.5   56.58 14.83  34  75    41 -0.05\nraises        5 30 64.63 10.40   63.5   64.50 11.12  43  88    45  0.20\ncritical      6 30 74.77  9.89   77.5   75.83  7.41  49  92    43 -0.87\nadvance       7 30 42.93 10.29   41.0   41.83  8.90  25  72    47  0.85\n           kurtosis   se\nrating        -0.77 2.22\ncomplaints    -0.68 2.43\nprivileges    -0.41 2.23\nlearning      -1.22 2.14\nraises        -0.60 1.90\ncritical       0.17 1.81\nadvance        0.47 1.88"
  },
  {
    "objectID": "reliability.html#are-the-items-tau-equivalent",
    "href": "reliability.html#are-the-items-tau-equivalent",
    "title": "5  Reliability",
    "section": "5.3 Are the items tau-equivalent?",
    "text": "5.3 Are the items tau-equivalent?\nWhen determining which internal consistency coefficient may be most appropriate for our measurement instrument, we can look at whether the items are tau equivalent (equivalent factor loadings for all items). A simple way to do so is the run a unidimensional EFA and inspect the factor loadings:\n\nfa(attitude)$loadings\n\n\nLoadings:\n           MR1  \nrating     0.758\ncomplaints 0.834\nprivileges 0.603\nlearning   0.789\nraises     0.841\ncritical   0.284\nadvance    0.491\n\n                 MR1\nSS loadings    3.285\nProportion Var 0.469\n\n\nBased on the loadings, do you think the items are tau equivalent? What does this mean for our choice of internal consistency coefficient?"
  },
  {
    "objectID": "reliability.html#coefficient-omega",
    "href": "reliability.html#coefficient-omega",
    "title": "5  Reliability",
    "section": "5.4 Coefficient Omega",
    "text": "5.4 Coefficient Omega\nWe will use the MBESS package to compute coefficient omega:\n\nci.reliability(attitude, type = \"omega\", \n               conf.level = 0.95, \n               interval.type = \"mlr\")\n\n$est\n[1] 0.8563268\n\n$se\n[1] 0.04619404\n\n$ci.lower\n[1] 0.7657882\n\n$ci.upper\n[1] 0.9468655\n\n$conf.level\n[1] 0.95\n\n$type\n[1] \"omega\"\n\n$interval.type\n[1] \"robust maximum likelihood (wald ci)\"\n\n\nHow internally consistent are the scores on this measurement instrument with this sample? Here is how you’d report the reliability: Internal consistency of the Attitudes survey was good, \\(\\omega\\) = .86 (SE = .05), 95% CI = [.77, .95].\nWe can use the estimated internal consistency to get an estimate of the overall standard error of measurement (sem):\n\n# compute the SD of the attitude sum scores\nsd_x &lt;- sd(rowSums(attitude))\n\n# compute sem: sd_x * sqrt(1 - reliability)\nsem &lt;- sd_x * sqrt(1 - 0.8563268)\nsem\n\n[1] 21.88914\n\n\nSo, the average size of the error scores is 21.89."
  },
  {
    "objectID": "reliability.html#cronbachs-alpha",
    "href": "reliability.html#cronbachs-alpha",
    "title": "5  Reliability",
    "section": "5.5 Cronbach’s Alpha",
    "text": "5.5 Cronbach’s Alpha\nWe will use the psych package to compute Cronbach’s alpha.\n\nattitude_alpha &lt;- alpha(attitude)\n\nNumber of categories should be increased  in order to count frequencies. \n\n\nThis function returns a bunch of output that we can look at, starting with some basic summary statistics:\n\nsummary(attitude_alpha)\n\n\nReliability analysis   \n raw_alpha std.alpha G6(smc) average_r S/N   ase mean  sd median_r\n      0.84      0.84    0.88      0.43 5.2 0.042   60 8.2     0.45\n\n\nWe can also look at how Cronbach’s alpha would change if specific items were removed from the instrument. This can help us identify items that are measured with more measurement error:\n\nattitude_alpha$alpha.drop\n\n           raw_alpha std.alpha   G6(smc) average_r      S/N   alpha se\nrating     0.8097602 0.8081915 0.8317701 0.4125442 4.213534 0.05244967\ncomplaints 0.7969175 0.7956468 0.8201749 0.3935404 3.893487 0.05653049\nprivileges 0.8278478 0.8230879 0.8661986 0.4367533 4.652525 0.04757251\nlearning   0.8030310 0.7983665 0.8367262 0.3975597 3.959493 0.05429802\nraises     0.7953866 0.7847196 0.8261984 0.3779228 3.645105 0.05558872\ncritical   0.8638723 0.8634716 0.8900481 0.5131641 6.324481 0.03839722\nadvance    0.8404649 0.8346265 0.8563876 0.4568621 5.046918 0.04287642\n                var.r     med.r\nrating     0.03484463 0.4454779\ncomplaints 0.03454755 0.4261169\nprivileges 0.05381799 0.5316198\nlearning   0.04457495 0.3768830\nraises     0.04790794 0.3432934\ncritical   0.03015342 0.5582882\nadvance    0.04811380 0.4933310\n\n\nAnd finally, we can find the 95% CI:\n\nattitude_alpha$feldt\n\n\n   95% confidence boundaries (Feldt)\n lower alpha upper\n  0.74  0.84  0.92\n\n\nHere is how you’d report the internal consistency using Cronbach’s alpha: Internal consistency of the Attitudes survey was good, \\(\\alpha\\) = .84 (SE = .04), 95% CI = [.74, .92].\nNote that you can also use the MBESS package to compute Cronbach’s alpha:\n\nci.reliability(attitude, type = \"alpha\", \n               conf.level = 0.95, \n               interval.type = \"feldt\")\n\n$est\n[1] 0.8431428\n\n$se\n[1] NA\n\n$ci.lower\n[1] 0.7393757\n\n$ci.upper\n[1] 0.9157731\n\n$conf.level\n[1] 0.95\n\n$type\n[1] \"alpha\"\n\n$interval.type\n[1] \"feldt\""
  },
  {
    "objectID": "reliability.html#split-half-reliability",
    "href": "reliability.html#split-half-reliability",
    "title": "5  Reliability",
    "section": "5.6 Split-Half Reliability",
    "text": "5.6 Split-Half Reliability\nWe can also use the psych package to estimate the split-half reliability. This function returns (among some other things) the minimum, maximum, and average split-half reliability. Ideally, we want those numbers to be close together and close to 1. If the minimum and maximum are far apart, it indicates that only some specific splits can be considered essentially parallel.\n\nsplitHalf(attitude)\n\nSplit half reliabilities  \nCall: splitHalf(r = attitude)\n\nMaximum split half reliability (lambda 4) =  0.89\nGuttman lambda 6                          =  0.88\nAverage split half reliability            =  0.82\nGuttman lambda 3 (alpha)                  =  0.84\nGuttman lambda 2                          =  0.85\nMinimum split half reliability  (beta)    =  0.68\nAverage interitem r =  0.43  with median =  0.45"
  },
  {
    "objectID": "reliability.html#disattenuation-of-correlations",
    "href": "reliability.html#disattenuation-of-correlations",
    "title": "5  Reliability",
    "section": "5.7 Disattenuation of correlations",
    "text": "5.7 Disattenuation of correlations\nAs discussed in class, when two measures are not perfectly reliable, then the correlation between them will be biased, or attenuated (i.e., lower than it should be). In this part of the lab, we’ll see this phenomenon in action.\nFirst, we’ll split the attitude survey in two parts, so we can look at the correlation between the two parts. In Assignment 6, you will do something similar, but for two different surveys. However, you will still need to split the Assignment 6 data frame into two parts, so the code below is still relevant.\n\n# Split attitude data into two parts \n# for demonstration\npart1 &lt;- attitude[,c(1:4)]\npart2 &lt;- attitude[,c(5:7)]\n\nTo compute a correlation between the two tests, we need to compute each participant’s sumscore across the items. We can use a function called rowSums() to do this:\n\n# Compute summed scores of \n# each part using rowSums()\nsumscore1 &lt;- rowSums(part1)\nsumscore2 &lt;- rowSums(part2)\n\nNow, we can compute the observed correlation of the summed scores:\n\n# Compute correlation between summed scores\nobscor &lt;- cor(sumscore1,sumscore2)\nobscor\n\n[1] 0.5438004\n\n\nBut we already know from our earlier assessment above that the full attitude test is not perfectly reliable. Now we also need to see if these two parts are perfectly reliable or not. To decide between using Cronbach’s alpha and coefficient omega, we need to assess the (lack of) tau equivalence of the two parts:\n\n#examine tau equivalence\nfa(part1)$loadings\n\n\nLoadings:\n           MR1  \nrating     0.855\ncomplaints 0.920\nprivileges 0.590\nlearning   0.713\n\n                 MR1\nSS loadings    2.434\nProportion Var 0.608\n\nfa(part2)$loadings\n\n\nLoadings:\n         MR1  \nraises   0.874\ncritical 0.431\nadvance  0.657\n\n                 MR1\nSS loadings    1.381\nProportion Var 0.460\n\n\nThese loadings do not look tau equivalent, so we will use coefficient omega to quantify the internal consistency. This time, we’re using the $ operator to extract just the omega estimate (est) from the ci.reliability() output:\n\n#record omega reliability estimates of both parts\nomega1 &lt;- ci.reliability(part1)$est\nomega2 &lt;- ci.reliability(part2)$est\n\nomega1\n\n[1] 0.8615586\n\nomega2\n\n[1] 0.7097988\n\n\nThe omegas above show that the two tests are not perfectly reliable, so it’s important to disattenuate the observed correlation:\n\n# Disattenuated correlation between tests\ndiscor &lt;- obscor / sqrt(omega1 * omega2)\ndiscor\n\n[1] 0.6953916\n\n\n\n\n\n\n\n\nNote\n\n\n\nA nice feature of CFA (or structural equation modeling more generally) is that correlations between factors are disattenuated for (lack of) reliability, because the factors only represent the true score part of the item’s variability, while the error variance is separated into the residual or error variance of the indicators."
  },
  {
    "objectID": "reliability.html#summary",
    "href": "reliability.html#summary",
    "title": "5  Reliability",
    "section": "5.8 Summary",
    "text": "5.8 Summary\nIn this R lab, you learned how to determine whether a set of items are tau-equivalent and how to compute coefficient omega and Cronbach’s alpha to evaluate internal consistency reliability. You also learned how to get the split-half reliability. Finally, you used the dissatenuation formula to dissatenuate a correlation for measurement error."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ESCP 8082 R Labs",
    "section": "",
    "text": "Preface\nWelcome to the R Labs page for ESCP 8082: Foundations of Educational and Psychological Measurement (University of Missouri). Although these labs were created for use in a classroom setting, they are available to all who happen to find them and find use in them. Please don’t hesitate to let me know about any bugs or errors you notice, by contacting me at sdwinter@missouri.edu\nYou can also download a PDF version of these R Labs. At the start of each specific R lab, you will also find a link to download just the R file with all the code used in the lab."
  },
  {
    "objectID": "reliability.html#loading-r-packages",
    "href": "reliability.html#loading-r-packages",
    "title": "5  Reliability",
    "section": "5.1 Loading R packages",
    "text": "5.1 Loading R packages\nIn this lab we will be using a new package: MBESS (Methods for the Behavioral, Educational, and Social Sciences), which includes a function that calculates coefficient omega and (more importantly) its confidence interval. Although other packages can also estimate coefficient omega, they often do not provide a confidence interval. We will also use the psych package. We can simply get those packages from the R package library:\n\nlibrary(psych)\nlibrary(MBESS)"
  },
  {
    "objectID": "measurementinvariance.html#loading-r-packages",
    "href": "measurementinvariance.html#loading-r-packages",
    "title": "6  Measurement Invariance",
    "section": "6.1 Loading R packages",
    "text": "6.1 Loading R packages\nIn this lab, we will use the lavaan and semTools packages, which we have already installed in earlier labs. So, we can simply get those packages from the R package library:\n\nlibrary(lavaan)\nlibrary(semTools)"
  }
]